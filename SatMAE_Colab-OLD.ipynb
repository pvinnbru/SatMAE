{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cae6c9",
   "metadata": {},
   "source": [
    "# SatMAE Finetuning on Google Colab\n",
    "\n",
    "This notebook sets up and runs SatMAE finetuning on EuroSAT dataset using Google Colab's free GPU.\n",
    "\n",
    "## üöÄ Features:\n",
    "- Automatic environment setup\n",
    "- EuroSAT dataset download and preprocessing\n",
    "- SatMAE model finetuning with multispectral data\n",
    "- GPU acceleration (T4/V100/A100)\n",
    "\n",
    "**Runtime**: Make sure to select **GPU** runtime (Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811e23a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Make sure to enable GPU runtime!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193823a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing required packages...\")\n",
    "!pip install timm  #==0.3.2\n",
    "!pip install rasterio\n",
    "!pip install wandb\n",
    "!pip install tensorboard\n",
    "!pip install gdown  # For Google Drive downloads\n",
    "\n",
    "# Import and check versions\n",
    "import timm\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import gdown\n",
    "\n",
    "print(f\"‚úÖ timm version: {timm.__version__}\")\n",
    "print(f\"‚úÖ rasterio version: {rasterio.__version__}\")\n",
    "print(f\"‚úÖ gdown installed for Google Drive downloads\")\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dec10",
   "metadata": {},
   "source": [
    "## 2. Download SatMAE Code and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the SatMAE repository\n",
    "!git clone https://github.com/pvinnbru/SatMAE.git\n",
    "%cd SatMAE\n",
    "\n",
    "# List repository contents\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58849c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data folder from Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy data folder from Google Drive to SatMAE/data\n",
    "source = '/content/drive/MyDrive/data'\n",
    "target = 'data'\n",
    "\n",
    "if os.path.exists(target):\n",
    "    shutil.rmtree(target)\n",
    "\n",
    "shutil.copytree(source, target)\n",
    "print(f\"‚úÖ Data copied from {source} to {target}\")\n",
    "\n",
    "# Verify contents\n",
    "print(\"\\nüìÅ Data structure:\")\n",
    "!ls -la data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0215ec",
   "metadata": {},
   "source": [
    "### üìÅ Google Drive Setup\n",
    "\n",
    "**Required Google Drive Structure:**\n",
    "```\n",
    "MyDrive/\n",
    "‚îú‚îÄ‚îÄ data/                           # Unzipped EuroSAT dataset folder\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ eurosat_ms/                 # Multispectral dataset\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ eurosat_rgb/                # RGB dataset  \n",
    "‚îî‚îÄ‚îÄ checkpoint/\n",
    "    ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth # Pretrained model checkpoint\n",
    "```\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Unzip your EuroSAT dataset and upload the `data/` folder to Google Drive root\n",
    "2. Upload checkpoint to `MyDrive/checkpoint/pretrain-vit-large-e199.pth`\n",
    "3. Run the cells - they will copy files to the local workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained checkpoint from Google Drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "drive_checkpoint_path = '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "local_checkpoint_dir = 'checkpoints'\n",
    "local_checkpoint_path = 'checkpoints/pretrain-vit-large-e199.pth'\n",
    "\n",
    "print(\"üîß Loading pretrained checkpoint from Google Drive...\")\n",
    "print(f\"Source: {drive_checkpoint_path}\")\n",
    "print(f\"Target: {local_checkpoint_path}\")\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs(local_checkpoint_dir, exist_ok=True)\n",
    "print(f\"üìÇ Created directory: {local_checkpoint_dir}/\")\n",
    "\n",
    "# Check if checkpoint exists in Google Drive\n",
    "if os.path.exists(drive_checkpoint_path):\n",
    "    print(f\"‚úÖ Found checkpoint in Google Drive\")\n",
    "    print(f\"üìä File size: {os.path.getsize(drive_checkpoint_path) / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Copy checkpoint to local directory\n",
    "    try:\n",
    "        shutil.copy2(drive_checkpoint_path, local_checkpoint_path)\n",
    "        print(f\"‚úÖ Checkpoint copied successfully!\")\n",
    "        print(f\"üìÅ Available at: {local_checkpoint_path}\")\n",
    "        \n",
    "        # Verify the file\n",
    "        if os.path.exists(local_checkpoint_path):\n",
    "            print(f\"‚úÖ Verification successful - checkpoint ready for training\")\n",
    "        else:\n",
    "            print(f\"‚ùå Verification failed - file not found at target location\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Copy failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint not found at: {drive_checkpoint_path}\")\n",
    "    print(\"Please ensure you have uploaded the checkpoint to your Google Drive\")\n",
    "    print(\"\\nTo fix this:\")\n",
    "    print(\"1. Go to your Google Drive\")\n",
    "    print(\"2. Create a folder called 'checkpoint' in the root directory\")\n",
    "    print(\"3. Upload 'pretrain-vit-large-e199.pth' to MyDrive/checkpoint/\")\n",
    "    print(\"4. Run this cell again\")\n",
    "    \n",
    "    print(f\"\\nExpected Google Drive structure:\")\n",
    "    print(f\"  MyDrive/\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ data.zip\")\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ checkpoint/\")\n",
    "    print(f\"      ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e0ad7",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73632f04",
   "metadata": {},
   "source": [
    "### **3.1 Generate txt Files and Training Subsets**\n",
    "\n",
    "The text files are used for loading Eurosat Data stored in `..\\data\\`. They look like this:\n",
    "\n",
    "```\n",
    "<path_to_image> <label>\n",
    "```\n",
    "For example:\n",
    "```\n",
    "/path/to/image1.tif    0\n",
    "/path/to/image2.tif    3\n",
    "...\n",
    "```\n",
    "\n",
    "The .txt-files are generate from the script below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val splits and subsets\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "def generate_split_txt(root_folder, out_txt_path, split_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Creates train/val .txt files from a root image folder organized by class.\n",
    "    Supports .tif and .jpg files.\n",
    "    \"\"\"\n",
    "    class_names = sorted(os.listdir(root_folder))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "\n",
    "    all_samples = []\n",
    "    for cls in class_names:\n",
    "        tif_paths = glob(os.path.join(root_folder, cls, \"*.tif\"))\n",
    "        jpg_paths = glob(os.path.join(root_folder, cls, \"*.jpg\"))\n",
    "        image_paths = tif_paths + jpg_paths\n",
    "        for path in image_paths:\n",
    "            all_samples.append(f\"{path} {class_to_idx[cls]}\")\n",
    "\n",
    "    if not all_samples:\n",
    "        print(f\"‚ö†Ô∏è  No image files found in: {root_folder}\")\n",
    "        return\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_samples)\n",
    "    split_idx = int(len(all_samples) * split_ratio)\n",
    "    train_samples = all_samples[:split_idx]\n",
    "    val_samples = all_samples[split_idx:]\n",
    "\n",
    "    with open(out_txt_path.replace(\".txt\", \"_train.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_samples))\n",
    "    with open(out_txt_path.replace(\".txt\", \"_val.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(val_samples))\n",
    "\n",
    "    print(f\"‚úÖ Created train/val splits for: {root_folder}\")\n",
    "    print(f\"   ‚Üí Train: {len(train_samples)} samples\")\n",
    "    print(f\"   ‚Üí Val:   {len(val_samples)} samples\")\n",
    "\n",
    "# Execution\n",
    "generate_split_txt(\"../data/eurosat_ms\", \"../data_splits/eurosat_ms.txt\")\n",
    "generate_split_txt(\"../data/eurosat_rgb\", \"../data_splits/eurosat_rgb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa5db5",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 **Create Training Subsets (10%, 25%, 50%, 100%)**\n",
    "\n",
    "The Goal is to measure how model performance improves as the training data size increases. To ensure fair and meaningful comparisons across runs, the validation set remains fixed.\n",
    "\n",
    "The following textfiles were generated and include the complete dataset:\n",
    "\n",
    "```\n",
    "../data_splits/eurosat_ms_train.txt\n",
    "../data_splits/eurosat_rgb_train.txt\n",
    "```\n",
    "\n",
    "To subsample:\n",
    "\n",
    "* Randomly select a percentage of lines from that file\n",
    "* Save them into new files like:\n",
    "\n",
    "  ```\n",
    "  ../data_splits/eurosat_ms_train_10.txt\n",
    "  ../data_splits/eurosat_ms_train_25.txt\n",
    "  ../data_splits/eurosat_ms_train_50.txt\n",
    "  ```\n",
    "\n",
    "Do this for RGB and MS too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_txt_file(input_path, output_prefix, percentages=[10, 25, 50], seed=42):\n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    for p in percentages:\n",
    "        count = int(len(lines) * (p / 100))\n",
    "        subset = lines[:count]\n",
    "        out_path = f\"{output_prefix}_{p}.txt\"\n",
    "        with open(out_path, 'w') as f_out:\n",
    "            f_out.writelines(subset)\n",
    "        print(f\"Saved {p}% subset to {out_path} ({count} samples)\")\n",
    "\n",
    "\n",
    "#Execution\n",
    "subsample_txt_file(\"../data_splits/eurosat_ms_train.txt\", \"../data_splits/eurosat_ms_train\", percentages=[10, 25, 50, 75])\n",
    "subsample_txt_file(\"../data_splits/eurosat_rgb_train.txt\", \"../data_splits/eurosat_rgb_train\", percentages=[10, 25, 50, 75])\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(\"\\nüìÅ Generated files:\")\n",
    "!ls -la data_splits/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5177448",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all required files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'main_finetune.py',\n",
    "    'data_splits/eurosat_ms_train_10.txt',\n",
    "    'data_splits/eurosat_ms_val.txt',\n",
    "    'checkpoints/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "print(\"Checking required files:\")\n",
    "all_good = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüöÄ All files ready for training!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some files are missing. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SatMAE finetuning\n",
    "# Adjust batch_size based on available GPU memory\n",
    "\n",
    "# Check GPU memory and adjust batch size accordingly\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "if gpu_memory_gb >= 15:  # A100, V100\n",
    "    batch_size = 16\n",
    "    accum_iter = 8\n",
    "elif gpu_memory_gb >= 11:  # T4 or similar\n",
    "    batch_size = 8\n",
    "    accum_iter = 16\n",
    "else:  # Smaller GPUs\n",
    "    batch_size = 4\n",
    "    accum_iter = 32\n",
    "\n",
    "print(f\"GPU Memory: {gpu_memory_gb:.1f}GB\")\n",
    "print(f\"Using batch_size={batch_size}, accum_iter={accum_iter}\")\n",
    "\n",
    "# Run training command\n",
    "training_cmd = f\"\"\"\n",
    "python main_finetune.py \\\n",
    "  --model_type group_c \\\n",
    "  --model vit_large_patch16 \\\n",
    "  --dataset_type euro_sat \\\n",
    "  --train_path data_splits/eurosat_ms_train_10.txt \\\n",
    "  --test_path data_splits/eurosat_ms_val.txt \\\n",
    "  --finetune checkpoints/pretrain-vit-large-e199.pth \\\n",
    "  --input_size 96 --patch_size 8 \\\n",
    "  --batch_size {batch_size} --accum_iter {accum_iter} \\\n",
    "  --epochs 30 --blr 2e-4 \\\n",
    "  --weight_decay 0.05 \\\n",
    "  --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n",
    "  --dropped_bands 0 9 10 \\\n",
    "  --num_workers 2 \\\n",
    "  --output_dir results/eurosat_ms_10 \\\n",
    "  --log_dir results/eurosat_ms_10\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take approximately 30-60 minutes depending on GPU\")\n",
    "print(\"Command:\")\n",
    "print(training_cmd)\n",
    "\n",
    "# Execute training\n",
    "!{training_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ea11f",
   "metadata": {},
   "source": [
    "## 5. Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb58fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/eurosat_ms_10\n",
    "\n",
    "print(\"TensorBoard is running above!\")\n",
    "print(\"You can monitor training progress, loss curves, and metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "import glob\n",
    "\n",
    "results_dir = \"results/eurosat_ms_10\"\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Training results:\")\n",
    "    !ls -la {results_dir}\n",
    "    \n",
    "    # Look for log files\n",
    "    log_files = glob.glob(f\"{results_dir}/*.txt\")\n",
    "    if log_files:\n",
    "        print(f\"\\nLatest log file: {log_files[-1]}\")\n",
    "        !tail -20 {log_files[-1]}\n",
    "    \n",
    "    # Look for checkpoints\n",
    "    checkpoints = glob.glob(f\"{results_dir}/*.pth\")\n",
    "    if checkpoints:\n",
    "        print(f\"\\nCheckpoints created: {len(checkpoints)}\")\n",
    "        for cp in checkpoints[-3:]:\n",
    "            print(f\"  {cp}\")\n",
    "else:\n",
    "    print(\"No results directory found. Training may not have started yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12aa25",
   "metadata": {},
   "source": [
    "## 6. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory\n",
    "        if os.path.exists('results'):\n",
    "            for root, dirs, files in os.walk('results'):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        # Add training logs\n",
    "        log_files = glob.glob('*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "    \n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"‚úÖ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3d0af",
   "metadata": {},
   "source": [
    "## 7. Optional: Cleanup and Additional Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments with different data percentages\n",
    "experiments = [25, 50, 75]\n",
    "\n",
    "for pct in experiments:\n",
    "    print(f\"\\n=== Running experiment with {pct}% of data ===\")\n",
    "    \n",
    "    # Adjust epochs based on data size\n",
    "    epochs = max(10, 30 - (pct // 25) * 5)  # Fewer epochs for more data\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    python main_finetune.py \\\n",
    "      --model_type group_c \\\n",
    "      --model vit_large_patch16 \\\n",
    "      --dataset_type euro_sat \\\n",
    "      --train_path data_splits/eurosat_ms_train_{pct}.txt \\\n",
    "      --test_path data_splits/eurosat_ms_val.txt \\\n",
    "      --finetune checkpoints/pretrain-vit-large-e199.pth \\\n",
    "      --input_size 96 --patch_size 8 \\\n",
    "      --batch_size {batch_size} --accum_iter {accum_iter} \\\n",
    "      --epochs {epochs} --blr 2e-4 \\\n",
    "      --weight_decay 0.05 \\\n",
    "      --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n",
    "      --dropped_bands 0 9 10 \\\n",
    "      --num_workers 2 \\\n",
    "      --output_dir results/eurosat_ms_{pct} \\\n",
    "      --log_dir results/eurosat_ms_{pct}\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Training with {pct}% data for {epochs} epochs...\")\n",
    "    !{cmd}\n",
    "    \n",
    "    print(f\"Completed {pct}% experiment\")\n",
    "\n",
    "print(\"\\nüéâ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up large files to save space\n",
    "print(\"Current disk usage:\")\n",
    "!df -h\n",
    "\n",
    "print(\"\\nLarge files and directories:\")\n",
    "!du -sh * | sort -hr\n",
    "\n",
    "# Uncomment to remove dataset after training\n",
    "# !rm -rf data/EuroSATallBands.zip\n",
    "# !rm -rf data/EuroSATallBands\n",
    "# print(\"Dataset files removed to save space\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
