{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2441a61c",
   "metadata": {},
   "source": [
    "# SatMAE Finetuning on Google Colab\n",
    "\n",
    "This notebook sets up and runs SatMAE finetuning on EuroSAT dataset using Google Colab's free GPU.\n",
    "\n",
    "## 🚀 Features:\n",
    "- Automatic SatMAE repository download\n",
    "- Conda environment setup using original env.yml\n",
    "- EuroSAT dataset integration via Google Drive\n",
    "- SatMAE model finetuning with multispectral data\n",
    "- GPU acceleration (T4/V100/A100)\n",
    "\n",
    "**Runtime**: Make sure to select **GPU** runtime (Runtime → Change runtime type → Hardware accelerator → GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cae6c9",
   "metadata": {
    "id": "c6cae6c9"
   },
   "source": [
    "## 1. Pull SatMAE Code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30d928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b30d928",
    "outputId": "5397e735-dc73-4597-a12c-305e314d3909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "CUDA version: 12.4\n",
      "GPU memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Clone SatMAE repository from GitHub\n",
    "import os\n",
    "\n",
    "print(\"\udce5 Downloading SatMAE repository from GitHub...\")\n",
    "\n",
    "if not os.path.exists('SatMAE'):\n",
    "    !git clone https://github.com/pvinnbru/SatMAE.git\n",
    "    print(\"✅ SatMAE repository cloned successfully\")\n",
    "else:\n",
    "    print(\"✅ SatMAE repository already exists\")\n",
    "\n",
    "# Navigate to SatMAE directory\n",
    "%cd SatMAE\n",
    "\n",
    "# List repository contents\n",
    "print(\"\\n\udcc2 Repository contents:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\n🎉 SatMAE code ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e4acf",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Installing conda environment using the original SatMAE env.yml file. This ensures exact package compatibility as tested by the SatMAE authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193823a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "193823a0",
    "outputId": "7811b6ae-27c6-478c-acbf-7d833cf2a146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.17)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.4)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.7.14)\n",
      "Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (1.4.3)\n",
      "Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.7.14)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1.2)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.7.14)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "✅ timm version: 1.0.17\n",
      "✅ rasterio version: 1.4.3\n",
      "✅ gdown installed for Google Drive downloads\n",
      "All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install conda environment using the original env.yml\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"\udd27 Setting up conda environment from env.yml...\")\n",
    "\n",
    "# Check if conda is available\n",
    "conda_available = subprocess.run(\"which conda\", shell=True, capture_output=True).returncode == 0\n",
    "\n",
    "if not conda_available:\n",
    "    print(\"📦 Installing Miniconda...\")\n",
    "    !wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
    "    !bash miniconda.sh -b -p /content/miniconda\n",
    "    \n",
    "    # Add conda to PATH\n",
    "    os.environ['PATH'] = '/content/miniconda/bin:' + os.environ['PATH']\n",
    "    \n",
    "    # Initialize conda\n",
    "    !source /content/miniconda/bin/activate && conda init bash\n",
    "    print(\"✅ Miniconda installed successfully\")\n",
    "else:\n",
    "    print(\"✅ Conda already available\")\n",
    "\n",
    "# Install mamba for faster package resolution\n",
    "print(\"📦 Installing mamba for faster package resolution...\")\n",
    "!conda install mamba -n base -c conda-forge -y\n",
    "\n",
    "# Create conda environment from env.yml\n",
    "print(\"🏗️ Creating conda environment from env.yml...\")\n",
    "print(\"⏳ This will take 5-10 minutes...\")\n",
    "!mamba env create -f env.yml\n",
    "\n",
    "print(\"✅ Conda environment created successfully!\")\n",
    "\n",
    "# Verify installation\n",
    "print(\"\\n🔍 Verifying environment...\")\n",
    "verification_cmd = '''\n",
    "source /content/miniconda/bin/activate sat_env && python -c \"\n",
    "import torch\n",
    "import torchvision \n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "print('✅ PyTorch:', torch.__version__)\n",
    "print('✅ torchvision:', torchvision.__version__)\n",
    "print('✅ timm:', timm.__version__)\n",
    "print('✅ numpy:', np.__version__)\n",
    "\n",
    "# Test critical SatMAE imports\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "print('✅ SatMAE-critical timm imports successful')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('✅ CUDA:', torch.cuda.get_device_name(0))\n",
    "    print('✅ GPU Memory: {:.1f} GB'.format(torch.cuda.get_device_properties(0).total_memory / 1e9))\n",
    "else:\n",
    "    print('⚠️ CUDA not available')\n",
    "\n",
    "print('🚀 Environment ready for SatMAE!')\n",
    "\"\n",
    "'''\n",
    "\n",
    "result = subprocess.run(verification_cmd, shell=True, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Warnings:\", result.stderr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎉 CONDA ENVIRONMENT READY!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rx3xbWYb4i2K",
   "metadata": {
    "id": "rx3xbWYb4i2K"
   },
   "source": [
    "## 3. Mount Google Drive\n",
    "\n",
    "**Required Google Drive Structure:**\n",
    "```\n",
    "MyDrive/\n",
    "├── data/                           # Unzipped EuroSAT dataset folder\n",
    "│   ├── eurosat_ms/                 # Multispectral dataset\n",
    "│   └── eurosat_rgb/                # RGB dataset  \n",
    "└── checkpoint/\n",
    "    └── pretrain-vit-large-e199.pth # Pretrained model checkpoint\n",
    "```\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Unzip your EuroSAT dataset and upload the `data/` folder to Google Drive root\n",
    "2. Upload checkpoint to `MyDrive/checkpoint/pretrain-vit-large-e199.pth`\n",
    "3. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wPDQjAL2uzJg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPDQjAL2uzJg",
    "outputId": "38149c2a-b6a9-4585-b5c8-aef732b45fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "✅ Data copied from /content/drive/MyDrive/data to SatMAE/data\n",
      "\n",
      "📁 Data structure:\n",
      "total 16\n",
      "drwx------  4 root root 4096 Jul 25 13:33 .\n",
      "drwxr-xr-x  8 root root 4096 Jul 25 13:47 ..\n",
      "drwx------ 12 root root 4096 Jul 25 13:33 eurosat_ms\n",
      "drwx------ 12 root root 4096 Jul 25 13:36 eurosat_rgb\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive and access data directly (no copying needed!)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define data paths - use Google Drive directly\n",
    "data_root = '/content/drive/MyDrive/data'\n",
    "eurosat_ms_path = os.path.join(data_root, 'eurosat_ms')\n",
    "eurosat_rgb_path = os.path.join(data_root, 'eurosat_rgb')\n",
    "\n",
    "print(\"🔍 Checking data availability in Google Drive...\")\n",
    "\n",
    "# Verify eurosat_ms exists\n",
    "if os.path.exists(eurosat_ms_path):\n",
    "    print(f\"✅ Found eurosat_ms at: {eurosat_ms_path}\")\n",
    "    ms_classes = os.listdir(eurosat_ms_path)\n",
    "    print(f\"   📊 Classes: {len(ms_classes)} ({', '.join(ms_classes[:3])}...)\")\n",
    "else:\n",
    "    print(f\"❌ eurosat_ms not found at: {eurosat_ms_path}\")\n",
    "\n",
    "# Verify eurosat_rgb exists  \n",
    "if os.path.exists(eurosat_rgb_path):\n",
    "    print(f\"✅ Found eurosat_rgb at: {eurosat_rgb_path}\")\n",
    "    rgb_classes = os.listdir(eurosat_rgb_path)\n",
    "    print(f\"   📊 Classes: {len(rgb_classes)} ({', '.join(rgb_classes[:3])}...)\")\n",
    "else:\n",
    "    print(f\"❌ eurosat_rgb not found at: {eurosat_rgb_path}\")\n",
    "\n",
    "# Create data_splits directory in SatMAE folder (for txt files)\n",
    "splits_dir = 'data_splits'\n",
    "os.makedirs(splits_dir, exist_ok=True)\n",
    "print(f\"📂 Created directory: {os.path.abspath(splits_dir)}/\")\n",
    "\n",
    "print(\"\\n🚀 Data access configured!\")\n",
    "print(\"💡 Using Google Drive data directly - no copying needed!\")\n",
    "print(f\"📁 MS data: {eurosat_ms_path}\")\n",
    "print(f\"📁 RGB data: {eurosat_rgb_path}\")\n",
    "print(f\"📁 Splits will be saved to: {os.path.abspath(splits_dir)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8df47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfe8df47",
    "outputId": "8ec5166d-33fa-4c37-81ec-438d11a4feae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Loading pretrained checkpoint from Google Drive...\n",
      "Source: /content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth\n",
      "Target: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "📂 Created directory: SatMAE/checkpoints/\n",
      "✅ Found checkpoint in Google Drive\n",
      "📊 File size: 298.8 MB\n",
      "✅ Checkpoint copied successfully!\n",
      "📁 Available at: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "✅ Verification successful - checkpoint ready for training\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained checkpoint from Google Drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths (we're already in SatMAE directory after %cd SatMAE)\n",
    "drive_checkpoint_path = '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "local_checkpoint_dir = 'checkpoints'  # Fixed: removed SatMAE/ prefix\n",
    "local_checkpoint_path = 'checkpoints/pretrain-vit-large-e199.pth'  # Fixed: removed SatMAE/ prefix\n",
    "\n",
    "print(\"🔧 Loading pretrained checkpoint from Google Drive...\")\n",
    "print(f\"Source: {drive_checkpoint_path}\")\n",
    "print(f\"Target: {local_checkpoint_path}\")\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs(local_checkpoint_dir, exist_ok=True)\n",
    "print(f\"📂 Created directory: {os.path.abspath(local_checkpoint_dir)}/\")\n",
    "\n",
    "# Check if checkpoint exists in Google Drive\n",
    "if os.path.exists(drive_checkpoint_path):\n",
    "    print(f\"✅ Found checkpoint in Google Drive\")\n",
    "    print(f\"📊 File size: {os.path.getsize(drive_checkpoint_path) / 1e6:.1f} MB\")\n",
    "\n",
    "    # Copy checkpoint to local directory\n",
    "    try:\n",
    "        shutil.copy2(drive_checkpoint_path, local_checkpoint_path)\n",
    "        print(f\"✅ Checkpoint copied successfully!\")\n",
    "        print(f\"📁 Available at: {os.path.abspath(local_checkpoint_path)}\")\n",
    "\n",
    "        # Verify the file\n",
    "        if os.path.exists(local_checkpoint_path):\n",
    "            print(f\"✅ Verification successful - checkpoint ready for training\")\n",
    "        else:\n",
    "            print(f\"❌ Verification failed - file not found at target location\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Copy failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Checkpoint not found at: {drive_checkpoint_path}\")\n",
    "    print(\"Please ensure you have uploaded the checkpoint to your Google Drive\")\n",
    "    print(\"\\nTo fix this:\")\n",
    "    print(\"1. Go to your Google Drive\")\n",
    "    print(\"2. Create a folder called 'checkpoint' in the root directory\")\n",
    "    print(\"3. Upload 'pretrain-vit-large-e199.pth' to MyDrive/checkpoint/\")\n",
    "    print(\"4. Run this cell again\")\n",
    "\n",
    "    print(f\"\\nExpected Google Drive structure:\")\n",
    "    print(f\"  MyDrive/\")\n",
    "    print(f\"  ├── data/\")\n",
    "    print(f\"  │   ├── eurosat_ms/\")\n",
    "    print(f\"  │   └── eurosat_rgb/\")\n",
    "    print(f\"  └── checkpoint/\")\n",
    "    print(f\"      └── pretrain-vit-large-e199.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e0ad7",
   "metadata": {
    "id": "a83e0ad7"
   },
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EjXuF-fw7KC-",
   "metadata": {
    "id": "EjXuF-fw7KC-"
   },
   "source": [
    "### 4.1 Generate txt Files and Training Subsets\n",
    "\n",
    "The text files are used for loading Eurosat Data stored in `SatMAE\\data\\`. They look like this:\n",
    "\n",
    "```\n",
    "<path_to_image> <label>\n",
    "```\n",
    "For example:\n",
    "```\n",
    "/path/to/image1.tif    0\n",
    "/path/to/image2.tif    3\n",
    "...\n",
    "```\n",
    "\n",
    "The .txt-files are generated from the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eq2bpdtd7LPU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq2bpdtd7LPU",
    "outputId": "4824e4eb-6e4b-4b7d-ad1d-c65021098206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created train/val splits for: SatMAE/data/eurosat_ms\n",
      "   → Train: 21600 samples\n",
      "   → Val:   5400 samples\n",
      "✅ Created train/val splits for: SatMAE/data/eurosat_rgb\n",
      "   → Train: 21600 samples\n",
      "   → Val:   5400 samples\n"
     ]
    }
   ],
   "source": [
    "# Create train/val splits and subsets using Google Drive data directly\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "def generate_split_txt(root_folder, out_txt_path, split_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Creates train/val .txt files from a root image folder organized by class.\n",
    "    Supports .tif and .jpg files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(root_folder):\n",
    "        print(f\"❌ Data folder not found: {root_folder}\")\n",
    "        return\n",
    "        \n",
    "    class_names = sorted(os.listdir(root_folder))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "\n",
    "    all_samples = []\n",
    "    for cls in class_names:\n",
    "        tif_paths = glob(os.path.join(root_folder, cls, \"*.tif\"))\n",
    "        jpg_paths = glob(os.path.join(root_folder, cls, \"*.jpg\"))\n",
    "        image_paths = tif_paths + jpg_paths\n",
    "        for path in image_paths:\n",
    "            all_samples.append(f\"{path} {class_to_idx[cls]}\")\n",
    "\n",
    "    if not all_samples:\n",
    "        print(f\"⚠️  No image files found in: {root_folder}\")\n",
    "        return\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_samples)\n",
    "    split_idx = int(len(all_samples) * split_ratio)\n",
    "    train_samples = all_samples[:split_idx]\n",
    "    val_samples = all_samples[split_idx:]\n",
    "\n",
    "    # Save to SatMAE/data_splits directory (maintain expected structure)\n",
    "    splits_dir = 'data_splits'\n",
    "    os.makedirs(splits_dir, exist_ok=True)\n",
    "    train_path = out_txt_path.replace(\".txt\", \"_train.txt\")\n",
    "    val_path = out_txt_path.replace(\".txt\", \"_val.txt\")\n",
    "    \n",
    "    with open(train_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_samples))\n",
    "    with open(val_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(val_samples))\n",
    "\n",
    "    print(f\"✅ Created train/val splits for: {root_folder}\")\n",
    "    print(f\"   → Train: {len(train_samples)} samples → {train_path}\")\n",
    "    print(f\"   → Val:   {len(val_samples)} samples → {val_path}\")\n",
    "\n",
    "# Use Google Drive paths directly\n",
    "eurosat_ms_path = '/content/drive/MyDrive/data/eurosat_ms'\n",
    "eurosat_rgb_path = '/content/drive/MyDrive/data/eurosat_rgb'\n",
    "\n",
    "print(\"🔄 Generating splits from Google Drive data...\")\n",
    "\n",
    "# Generate splits using Google Drive data directly, save to SatMAE/data_splits/\n",
    "generate_split_txt(eurosat_ms_path, \"data_splits/eurosat_ms.txt\")\n",
    "generate_split_txt(eurosat_rgb_path, \"data_splits/eurosat_rgb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SZf5KTQy7NJN",
   "metadata": {
    "id": "SZf5KTQy7NJN"
   },
   "source": [
    "\n",
    "### 3.2 **Create Training Subsets (10%, 25%, 50%, 100%)**\n",
    "\n",
    "The Goal is to measure how model performance improves as the training data size increases. To ensure fair and meaningful comparisons across runs, the validation set remains fixed.\n",
    "\n",
    "The following textfiles were generated and include the complete dataset:\n",
    "\n",
    "```\n",
    "SatMAE/data_splits/eurosat_ms_train.txt\n",
    "SatMAE/data_splits/eurosat_rgb_train.txt\n",
    "```\n",
    "\n",
    "To subsample:\n",
    "\n",
    "* Randomly select a percentage of lines from that file\n",
    "* Save them into new files like:\n",
    "\n",
    "  ```\n",
    "  SatMAE/data_splits/eurosat_ms_train_10.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_25.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_50.txt\n",
    "  ```\n",
    "\n",
    "Do this for RGB and MS too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sb7t6GON7Pmi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sb7t6GON7Pmi",
    "outputId": "14347e08-e565-4e32-a86a-db62b6c5c23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10% subset to SatMAE/data_splits/eurosat_ms_train_10.txt (2160 samples)\n",
      "Saved 25% subset to SatMAE/data_splits/eurosat_ms_train_25.txt (5400 samples)\n",
      "Saved 50% subset to SatMAE/data_splits/eurosat_ms_train_50.txt (10800 samples)\n",
      "Saved 75% subset to SatMAE/data_splits/eurosat_ms_train_75.txt (16200 samples)\n",
      "Saved 10% subset to SatMAE/data_splits/eurosat_rgb_train_10.txt (2160 samples)\n",
      "Saved 25% subset to SatMAE/data_splits/eurosat_rgb_train_25.txt (5400 samples)\n",
      "Saved 50% subset to SatMAE/data_splits/eurosat_rgb_train_50.txt (10800 samples)\n",
      "Saved 75% subset to SatMAE/data_splits/eurosat_rgb_train_75.txt (16200 samples)\n",
      "\n",
      "✅ Data preprocessing complete!\n",
      "\n",
      "📁 Generated files:\n",
      "total 6732\n",
      "drwxr-xr-x 3 root root    4096 Jul 25 14:14 .\n",
      "drwxr-xr-x 9 root root    4096 Jul 25 14:01 ..\n",
      "-rw-r--r-- 1 root root  119137 Jul 25 14:15 eurosat_ms_train_10.txt\n",
      "-rw-r--r-- 1 root root  298594 Jul 25 14:15 eurosat_ms_train_25.txt\n",
      "-rw-r--r-- 1 root root  595927 Jul 25 14:15 eurosat_ms_train_50.txt\n",
      "-rw-r--r-- 1 root root  893882 Jul 25 14:15 eurosat_ms_train_75.txt\n",
      "-rw-r--r-- 1 root root 1191419 Jul 25 14:14 eurosat_ms_train.txt\n",
      "-rw-r--r-- 1 root root  296509 Jul 25 14:14 eurosat_ms_val.txt\n",
      "-rw-r--r-- 1 root root  121255 Jul 25 14:15 eurosat_rgb_train_10.txt\n",
      "-rw-r--r-- 1 root root  303874 Jul 25 14:15 eurosat_rgb_train_25.txt\n",
      "-rw-r--r-- 1 root root  606632 Jul 25 14:15 eurosat_rgb_train_50.txt\n",
      "-rw-r--r-- 1 root root  910021 Jul 25 14:15 eurosat_rgb_train_75.txt\n",
      "-rw-r--r-- 1 root root 1212992 Jul 25 14:14 eurosat_rgb_train.txt\n",
      "-rw-r--r-- 1 root root  301936 Jul 25 14:14 eurosat_rgb_val.txt\n",
      "drwxr-xr-x 2 root root    4096 Jul 25 14:13 .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "def subsample_txt_file(input_path, output_prefix, percentages=[10, 25, 50], seed=42):\n",
    "    \"\"\"Subsample training data to create different dataset sizes.\"\"\"\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"❌ Input file not found: {input_path}\")\n",
    "        return\n",
    "        \n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    for p in percentages:\n",
    "        count = int(len(lines) * (p / 100))\n",
    "        subset = lines[:count]\n",
    "        out_path = f\"{output_prefix}_{p}.txt\"\n",
    "        with open(out_path, 'w') as f_out:\n",
    "            f_out.writelines(subset)\n",
    "        print(f\"Saved {p}% subset to {out_path} ({count} samples)\")\n",
    "\n",
    "print(\"🔄 Creating training subsets...\")\n",
    "\n",
    "# Use the generated training files in SatMAE/data_splits/\n",
    "subsample_txt_file(\"data_splits/eurosat_ms_train.txt\", \"data_splits/eurosat_ms_train\", percentages=[10, 25, 50, 75])\n",
    "subsample_txt_file(\"data_splits/eurosat_rgb_train.txt\", \"data_splits/eurosat_rgb_train\", percentages=[10, 25, 50, 75])\n",
    "\n",
    "print(\"\\n✅ Data preprocessing complete!\")\n",
    "print(\"\\n📁 Generated files:\")\n",
    "!ls -la data_splits/\n",
    "\n",
    "print(\"\\n💡 All txt files contain Google Drive paths:\")\n",
    "print(\"   Example paths point to /content/drive/MyDrive/data/eurosat_ms/...\")\n",
    "print(\"   No local data copying needed! 🚀\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5177448",
   "metadata": {
    "id": "f5177448"
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616c4e0",
   "metadata": {
    "id": "e616c4e0"
   },
   "outputs": [],
   "source": [
    "# Verify all required files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'main_finetune.py',  # We're in SatMAE directory\n",
    "    'data_splits/eurosat_ms_train_10.txt',  # Local txt files\n",
    "    'data_splits/eurosat_ms_val.txt',\n",
    "    'checkpoints/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "# Also check Google Drive data access\n",
    "gdrive_paths = [\n",
    "    '/content/drive/MyDrive/data/eurosat_ms',\n",
    "    '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "print(\"Checking required files:\")\n",
    "all_good = True\n",
    "\n",
    "# Check local files\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"✅ {file}\")\n",
    "    else:\n",
    "        print(f\"❌ {file} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "# Check Google Drive access\n",
    "print(\"\\nChecking Google Drive data access:\")\n",
    "for path in gdrive_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✅ {path}\")\n",
    "    else:\n",
    "        print(f\"❌ {path} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "# Verify txt files contain valid paths\n",
    "if os.path.exists('data_splits/eurosat_ms_train_10.txt'):\n",
    "    with open('data_splits/eurosat_ms_train_10.txt', 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        if first_line:\n",
    "            image_path = first_line.split()[0]\n",
    "            if os.path.exists(image_path):\n",
    "                print(f\"✅ Sample image accessible: {image_path}\")\n",
    "            else:\n",
    "                print(f\"❌ Sample image not accessible: {image_path}\")\n",
    "                all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n🚀 All files ready for training!\")\n",
    "    print(\"💡 Using Google Drive data directly - fast and efficient!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Some files are missing. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe43a7",
   "metadata": {
    "id": "a7fe43a7"
   },
   "outputs": [],
   "source": [
    "# Run SatMAE finetuning in conda environment\n",
    "import subprocess\n",
    "\n",
    "# Get GPU info in conda environment\n",
    "gpu_cmd = \"\"\"\n",
    "source /content/miniconda/bin/activate sat_env && python -c \"\n",
    "import torch\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "print(f'{gpu_memory_gb:.1f}')\n",
    "\"\n",
    "\"\"\"\n",
    "\n",
    "result = subprocess.run(gpu_cmd, shell=True, capture_output=True, text=True)\n",
    "gpu_memory_gb = float(result.stdout.strip()) if result.stdout.strip() else 0\n",
    "\n",
    "# Adjust batch_size based on available GPU memory\n",
    "if gpu_memory_gb >= 15:  # A100, V100\n",
    "    batch_size = 16\n",
    "    accum_iter = 8\n",
    "elif gpu_memory_gb >= 11:  # T4 or similar\n",
    "    batch_size = 8\n",
    "    accum_iter = 16\n",
    "else:  # Smaller GPUs or CPU\n",
    "    batch_size = 4\n",
    "    accum_iter = 32\n",
    "\n",
    "print(f\"🚀 GPU Memory: {gpu_memory_gb:.1f}GB\")\n",
    "print(f\"🚀 Using batch_size={batch_size}, accum_iter={accum_iter}\")\n",
    "\n",
    "# Training command using conda environment\n",
    "training_cmd = f\"\"\"source /content/miniconda/bin/activate sat_env && python main_finetune.py \\\\\n",
    "  --model_type group_c \\\\\n",
    "  --model vit_large_patch16 \\\\\n",
    "  --dataset_type euro_sat \\\\\n",
    "  --train_path data_splits/eurosat_ms_train_10.txt \\\\\n",
    "  --test_path data_splits/eurosat_ms_val.txt \\\\\n",
    "  --finetune checkpoints/pretrain-vit-large-e199.pth \\\\\n",
    "  --input_size 96 --patch_size 8 \\\\\n",
    "  --batch_size {batch_size} --accum_iter {accum_iter} \\\\\n",
    "  --epochs 30 --blr 2e-4 \\\\\n",
    "  --weight_decay 0.05 \\\\\n",
    "  --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\\\n",
    "  --dropped_bands 0 9 10 \\\\\n",
    "  --num_workers 2 \\\\\n",
    "  --output_dir results/eurosat_ms_10 \\\\\n",
    "  --log_dir results/eurosat_ms_10\"\"\"\n",
    "\n",
    "print(\"\\n🚀 Starting SatMAE training in conda environment...\")\n",
    "print(\"💡 Using Google Drive data directly (no copying needed!)\")\n",
    "print(\"✅ Running with exact tested package versions (timm 0.3.2, PyTorch 1.11.0)\")\n",
    "print(\"⏱️ This will take approximately 30-60 minutes depending on GPU\")\n",
    "print(\"\\nCommand:\")\n",
    "print(training_cmd.replace(\" &&\", \" &&\\n \"))\n",
    "\n",
    "# Execute training\n",
    "result = subprocess.run(training_cmd, shell=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n🎉 Training completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Training failed with return code: {result.returncode}\")\n",
    "    print(\"Check the output above for error details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ea11f",
   "metadata": {
    "id": "938ea11f"
   },
   "source": [
    "## 5. Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb58fcc",
   "metadata": {
    "id": "feb58fcc"
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir SatMAE/results/eurosat_ms_10\n",
    "\n",
    "print(\"TensorBoard is running above!\")\n",
    "print(\"You can monitor training progress, loss curves, and metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7a383",
   "metadata": {
    "id": "64c7a383"
   },
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "import glob\n",
    "\n",
    "results_dir = \"SatMAE/results/eurosat_ms_10\"\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Training results:\")\n",
    "    !ls -la {results_dir}\n",
    "\n",
    "    # Look for log files\n",
    "    log_files = glob.glob(f\"{results_dir}/*.txt\")\n",
    "    if log_files:\n",
    "        print(f\"\\nLatest log file: {log_files[-1]}\")\n",
    "        !tail -20 {log_files[-1]}\n",
    "\n",
    "    # Look for checkpoints\n",
    "    checkpoints = glob.glob(f\"{results_dir}/*.pth\")\n",
    "    if checkpoints:\n",
    "        print(f\"\\nCheckpoints created: {len(checkpoints)}\")\n",
    "        for cp in checkpoints[-3:]:\n",
    "            print(f\"  {cp}\")\n",
    "else:\n",
    "    print(\"No results directory found. Training may not have started yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12aa25",
   "metadata": {
    "id": "ff12aa25"
   },
   "source": [
    "## 6. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de651d",
   "metadata": {
    "id": "41de651d"
   },
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory from SatMAE\n",
    "        results_path = 'SatMAE/results'\n",
    "        if os.path.exists(results_path):\n",
    "            for root, dirs, files in os.walk(results_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add training logs from SatMAE directory\n",
    "        log_files = glob.glob('SatMAE/*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "\n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('SatMAE/results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"✅ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3d0af",
   "metadata": {
    "id": "b2b3d0af"
   },
   "source": [
    "## 7. Optional: Cleanup and Additional Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758d3fd",
   "metadata": {
    "id": "a758d3fd"
   },
   "outputs": [],
   "source": [
    "# Run experiments with different data percentages\n",
    "experiments = [25, 50, 75]\n",
    "\n",
    "for pct in experiments:\n",
    "    print(f\"\\n=== Running experiment with {pct}% of data ===\")\n",
    "\n",
    "    # Adjust epochs based on data size\n",
    "    epochs = max(10, 30 - (pct // 25) * 5)  # Fewer epochs for more data\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    cd SatMAE && python main_finetune.py \\\n",
    "      --model_type group_c \\\n",
    "      --model vit_large_patch16 \\\n",
    "      --dataset_type euro_sat \\\n",
    "      --train_path data_splits/eurosat_ms_train_{pct}.txt \\\n",
    "      --test_path data_splits/eurosat_ms_val.txt \\\n",
    "      --finetune checkpoints/pretrain-vit-large-e199.pth \\\n",
    "      --input_size 96 --patch_size 8 \\\n",
    "      --batch_size {batch_size} --accum_iter {accum_iter} \\\n",
    "      --epochs {epochs} --blr 2e-4 \\\n",
    "      --weight_decay 0.05 \\\n",
    "      --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n",
    "      --dropped_bands 0 9 10 \\\n",
    "      --num_workers 2 \\\n",
    "      --output_dir results/eurosat_ms_{pct} \\\n",
    "      --log_dir results/eurosat_ms_{pct}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training with {pct}% data for {epochs} epochs...\")\n",
    "    !{cmd}\n",
    "\n",
    "    print(f\"Completed {pct}% experiment\")\n",
    "\n",
    "print(\"\\n🎉 All experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f0b0d",
   "metadata": {
    "id": "fb9f0b0d"
   },
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory from SatMAE\n",
    "        results_path = 'SatMAE/results'\n",
    "        if os.path.exists(results_path):\n",
    "            for root, dirs, files in os.walk(results_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add training logs from SatMAE directory\n",
    "        log_files = glob.glob('SatMAE/*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "\n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('SatMAE/results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"✅ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
