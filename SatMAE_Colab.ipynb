{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cae6c9",
   "metadata": {
    "id": "c6cae6c9"
   },
   "source": [
    "# SatMAE Finetuning on Google Colab\n",
    "\n",
    "This notebook sets up and runs SatMAE finetuning on EuroSAT dataset using Google Colab's free GPU.\n",
    "\n",
    "## üöÄ Features:\n",
    "- Automatic environment setup\n",
    "- EuroSAT dataset download and preprocessing\n",
    "- SatMAE model finetuning with multispectral data\n",
    "- GPU acceleration (T4/V100/A100)\n",
    "\n",
    "**Runtime**: Make sure to select **GPU** runtime (Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623b87a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30d928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b30d928",
    "outputId": "5397e735-dc73-4597-a12c-305e314d3909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "CUDA version: 12.4\n",
      "GPU memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability (without importing torch yet)\n",
    "print(\"üîç Checking Google Colab GPU availability...\")\n",
    "\n",
    "# Check if CUDA is available in the system\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ NVIDIA GPU detected in system\")\n",
    "        # Extract GPU name from nvidia-smi output\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'Tesla' in line or 'RTX' in line or 'GTX' in line or 'T4' in line or 'V100' in line or 'A100' in line:\n",
    "                gpu_name = line.split('|')[1].strip()\n",
    "                print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No NVIDIA GPU detected\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not detect GPU\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: PyTorch will be installed in the next cell with correct versions\")\n",
    "print(\"üìã Runtime requirement: Make sure to select **GPU** runtime\")\n",
    "print(\"   (Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU)\")\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"\\nüêç Python version: {sys.version}\")\n",
    "print(\"‚úÖ Ready for SatMAE package installation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b55110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages with optimized versions for compatibility\n",
    "print(\"üöÄ Setting up SatMAE environment...\")\n",
    "print(\"üì¶ Installing packages (fast pip approach)...\")\n",
    "\n",
    "# Install packages with version constraints for compatibility\n",
    "!pip install 'numpy<2.0' --quiet  # Avoid NumPy 2.0 compatibility issues\n",
    "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
    "!pip install timm==0.9.12 --quiet  # Modern timm version with compatibility patches\n",
    "!pip install rasterio --quiet      # For satellite image processing (.tif files)\n",
    "!pip install wandb --quiet         # For experiment tracking\n",
    "!pip install tensorboard --quiet   # For training monitoring\n",
    "!pip install pandas --quiet        # For data handling\n",
    "!pip install pillow --quiet        # For image processing\n",
    "!pip install matplotlib --quiet    # For visualization\n",
    "!pip install tqdm --quiet          # For progress bars\n",
    "!pip install pyyaml --quiet        # For config files\n",
    "!pip install scikit-learn --quiet  # For data preprocessing\n",
    "\n",
    "print(\"\\nÔøΩ Applying compatibility patches...\")\n",
    "\n",
    "# Patch 1: Fix torch._six import error\n",
    "import sys\n",
    "import types\n",
    "six_module = types.ModuleType('six')\n",
    "six_module.PY3 = True\n",
    "six_module.string_types = str\n",
    "sys.modules['torch._six'] = six_module\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(\"üöÄ Using fast pip installation with modern PyTorch + compatibility patches\")\n",
    "print(\"üìã All required packages installed for SatMAE finetuning\")\n",
    "\n",
    "# Download the SatMAE repository\n",
    "import os\n",
    "if not os.path.exists('SatMAE'):\n",
    "    print(\"üì• Downloading SatMAE repository...\")\n",
    "    !git clone https://github.com/pvinnbru/SatMAE.git\n",
    "    print(\"‚úÖ Repository downloaded\")\n",
    "\n",
    "# Verify critical packages and imports\n",
    "print(\"\\nüîç Verifying installation...\")\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import timm\n",
    "    import numpy as np\n",
    "    import rasterio\n",
    "    import pandas as pd\n",
    "    import wandb\n",
    "    \n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"‚úÖ torchvision: {torchvision.__version__}\")\n",
    "    print(f\"‚úÖ timm: {timm.__version__}\")\n",
    "    print(f\"‚úÖ numpy: {np.__version__}\")\n",
    "    print(f\"‚úÖ rasterio: {rasterio.__version__}\")\n",
    "    \n",
    "    # Test critical SatMAE imports with compatibility\n",
    "    from timm.models.layers import trunc_normal_\n",
    "    from timm.data.mixup import Mixup\n",
    "    from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "    print(\"‚úÖ SatMAE-critical timm imports successful\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CUDA not available\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ FAST PIP ENVIRONMENT READY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úÖ Modern PyTorch 2.0.1 + compatibility patches\")\n",
    "    print(\"‚úÖ timm 0.9.12 with backward compatibility\")\n",
    "    print(\"‚úÖ All packages for satellite image processing\")\n",
    "    print(\"‚ö° Installation time: ~2 minutes (vs 10-15 for conda)\")\n",
    "    print(\"üöÄ Ready for SatMAE finetuning!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Some packages may not have installed correctly.\")\n",
    "\n",
    "\n",
    "# Navigate to SatMAE directory for subsequent cells\n",
    "%cd SatMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rx3xbWYb4i2K",
   "metadata": {
    "id": "rx3xbWYb4i2K"
   },
   "source": [
    "**Required Google Drive Structure:**\n",
    "```\n",
    "MyDrive/\n",
    "‚îú‚îÄ‚îÄ data/                           # Unzipped EuroSAT dataset folder\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ eurosat_ms/                 # Multispectral dataset\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ eurosat_rgb/                # RGB dataset  \n",
    "‚îî‚îÄ‚îÄ checkpoint/\n",
    "    ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth # Pretrained model checkpoint\n",
    "```\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Unzip your EuroSAT dataset and upload the `data/` folder to Google Drive root\n",
    "2. Upload checkpoint to `MyDrive/checkpoint/pretrain-vit-large-e199.pth`\n",
    "3. Run the cells - they will copy files to the local workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8df47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfe8df47",
    "outputId": "8ec5166d-33fa-4c37-81ec-438d11a4feae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading pretrained checkpoint from Google Drive...\n",
      "Source: /content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth\n",
      "Target: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "üìÇ Created directory: SatMAE/checkpoints/\n",
      "‚úÖ Found checkpoint in Google Drive\n",
      "üìä File size: 298.8 MB\n",
      "‚úÖ Checkpoint copied successfully!\n",
      "üìÅ Available at: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "‚úÖ Verification successful - checkpoint ready for training\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained checkpoint from Google Drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths (we're already in SatMAE directory after %cd SatMAE)\n",
    "drive_checkpoint_path = '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "local_checkpoint_dir = 'checkpoints'  # Fixed: removed SatMAE/ prefix\n",
    "local_checkpoint_path = 'checkpoints/pretrain-vit-large-e199.pth'  # Fixed: removed SatMAE/ prefix\n",
    "\n",
    "print(\"üîß Loading pretrained checkpoint from Google Drive...\")\n",
    "print(f\"Source: {drive_checkpoint_path}\")\n",
    "print(f\"Target: {local_checkpoint_path}\")\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs(local_checkpoint_dir, exist_ok=True)\n",
    "print(f\"üìÇ Created directory: {os.path.abspath(local_checkpoint_dir)}/\")\n",
    "\n",
    "# Check if checkpoint exists in Google Drive\n",
    "if os.path.exists(drive_checkpoint_path):\n",
    "    print(f\"‚úÖ Found checkpoint in Google Drive\")\n",
    "    print(f\"üìä File size: {os.path.getsize(drive_checkpoint_path) / 1e6:.1f} MB\")\n",
    "\n",
    "    # Copy checkpoint to local directory\n",
    "    try:\n",
    "        shutil.copy2(drive_checkpoint_path, local_checkpoint_path)\n",
    "        print(f\"‚úÖ Checkpoint copied successfully!\")\n",
    "        print(f\"üìÅ Available at: {os.path.abspath(local_checkpoint_path)}\")\n",
    "\n",
    "        # Verify the file\n",
    "        if os.path.exists(local_checkpoint_path):\n",
    "            print(f\"‚úÖ Verification successful - checkpoint ready for training\")\n",
    "        else:\n",
    "            print(f\"‚ùå Verification failed - file not found at target location\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Copy failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint not found at: {drive_checkpoint_path}\")\n",
    "    print(\"Please ensure you have uploaded the checkpoint to your Google Drive\")\n",
    "    print(\"\\nTo fix this:\")\n",
    "    print(\"1. Go to your Google Drive\")\n",
    "    print(\"2. Create a folder called 'checkpoint' in the root directory\")\n",
    "    print(\"3. Upload 'pretrain-vit-large-e199.pth' to MyDrive/checkpoint/\")\n",
    "    print(\"4. Run this cell again\")\n",
    "\n",
    "    print(f\"\\nExpected Google Drive structure:\")\n",
    "    print(f\"  MyDrive/\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ data/\")\n",
    "    print(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ eurosat_ms/\")\n",
    "    print(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ eurosat_rgb/\")\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ checkpoint/\")\n",
    "    print(f\"      ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EjXuF-fw7KC-",
   "metadata": {
    "id": "EjXuF-fw7KC-"
   },
   "source": [
    "### **3.1 Generate txt Files and Training Subsets**\n",
    "\n",
    "The text files are used for loading Eurosat Data stored in `SatMAE\\data\\`. They look like this:\n",
    "\n",
    "```\n",
    "<path_to_image> <label>\n",
    "```\n",
    "For example:\n",
    "```\n",
    "/path/to/image1.tif    0\n",
    "/path/to/image2.tif    3\n",
    "...\n",
    "```\n",
    "\n",
    "The .txt-files are generate from the script below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SZf5KTQy7NJN",
   "metadata": {
    "id": "SZf5KTQy7NJN"
   },
   "source": [
    "\n",
    "### 3.2 **Create Training Subsets (10%, 25%, 50%, 100%)**\n",
    "\n",
    "The Goal is to measure how model performance improves as the training data size increases. To ensure fair and meaningful comparisons across runs, the validation set remains fixed.\n",
    "\n",
    "The following textfiles were generated and include the complete dataset:\n",
    "\n",
    "```\n",
    "SatMAE/data_splits/eurosat_ms_train.txt\n",
    "SatMAE/data_splits/eurosat_rgb_train.txt\n",
    "```\n",
    "\n",
    "To subsample:\n",
    "\n",
    "* Randomly select a percentage of lines from that file\n",
    "* Save them into new files like:\n",
    "\n",
    "  ```\n",
    "  SatMAE/data_splits/eurosat_ms_train_10.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_25.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_50.txt\n",
    "  ```\n",
    "\n",
    "Do this for RGB and MS too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616c4e0",
   "metadata": {
    "id": "e616c4e0"
   },
   "outputs": [],
   "source": [
    "# Verify all required files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'main_finetune.py',  # We're in SatMAE directory\n",
    "    'data_splits/eurosat_ms_train_10.txt',  # Local txt files\n",
    "    'data_splits/eurosat_ms_val.txt',\n",
    "    'checkpoints/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "# Also check Google Drive data access\n",
    "gdrive_paths = [\n",
    "    '/content/drive/MyDrive/data/eurosat_ms',\n",
    "    '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "print(\"Checking required files:\")\n",
    "all_good = True\n",
    "\n",
    "# Check local files\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "# Check Google Drive access\n",
    "print(\"\\nChecking Google Drive data access:\")\n",
    "for path in gdrive_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {path} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "# Verify txt files contain valid paths\n",
    "if os.path.exists('data_splits/eurosat_ms_train_10.txt'):\n",
    "    with open('data_splits/eurosat_ms_train_10.txt', 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        if first_line:\n",
    "            image_path = first_line.split()[0]\n",
    "            if os.path.exists(image_path):\n",
    "                print(f\"‚úÖ Sample image accessible: {image_path}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Sample image not accessible: {image_path}\")\n",
    "                all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüöÄ All files ready for training!\")\n",
    "    print(\"üí° Using Google Drive data directly - fast and efficient!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some files are missing. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ea11f",
   "metadata": {
    "id": "938ea11f"
   },
   "source": [
    "## 5. Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7a383",
   "metadata": {
    "id": "64c7a383"
   },
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "import glob\n",
    "\n",
    "results_dir = \"SatMAE/results/eurosat_ms_10\"\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Training results:\")\n",
    "    !ls -la {results_dir}\n",
    "\n",
    "    # Look for log files\n",
    "    log_files = glob.glob(f\"{results_dir}/*.txt\")\n",
    "    if log_files:\n",
    "        print(f\"\\nLatest log file: {log_files[-1]}\")\n",
    "        !tail -20 {log_files[-1]}\n",
    "\n",
    "    # Look for checkpoints\n",
    "    checkpoints = glob.glob(f\"{results_dir}/*.pth\")\n",
    "    if checkpoints:\n",
    "        print(f\"\\nCheckpoints created: {len(checkpoints)}\")\n",
    "        for cp in checkpoints[-3:]:\n",
    "            print(f\"  {cp}\")\n",
    "else:\n",
    "    print(\"No results directory found. Training may not have started yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de651d",
   "metadata": {
    "id": "41de651d"
   },
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory from SatMAE\n",
    "        results_path = 'SatMAE/results'\n",
    "        if os.path.exists(results_path):\n",
    "            for root, dirs, files in os.walk(results_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add training logs from SatMAE directory\n",
    "        log_files = glob.glob('SatMAE/*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "\n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('SatMAE/results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"‚úÖ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758d3fd",
   "metadata": {
    "id": "a758d3fd"
   },
   "outputs": [],
   "source": [
    "# Run experiments with different data percentages\n",
    "experiments = [25, 50, 75]\n",
    "\n",
    "for pct in experiments:\n",
    "    print(f\"\\n=== Running experiment with {pct}% of data ===\")\n",
    "\n",
    "    # Adjust epochs based on data size\n",
    "    epochs = max(10, 30 - (pct // 25) * 5)  # Fewer epochs for more data\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    cd SatMAE && python main_finetune.py \\\n",
    "      --model_type group_c \\\n",
    "      --model vit_large_patch16 \\\n",
    "      --dataset_type euro_sat \\\n",
    "      --train_path data_splits/eurosat_ms_train_{pct}.txt \\\n",
    "      --test_path data_splits/eurosat_ms_val.txt \\\n",
    "      --finetune checkpoints/pretrain-vit-large-e199.pth \\\n",
    "      --input_size 96 --patch_size 8 \\\n",
    "      --batch_size {batch_size} --accum_iter {accum_iter} \\\n",
    "      --epochs {epochs} --blr 2e-4 \\\n",
    "      --weight_decay 0.05 \\\n",
    "      --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n",
    "      --dropped_bands 0 9 10 \\\n",
    "      --num_workers 2 \\\n",
    "      --output_dir results/eurosat_ms_{pct} \\\n",
    "      --log_dir results/eurosat_ms_{pct}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training with {pct}% data for {epochs} epochs...\")\n",
    "    !{cmd}\n",
    "\n",
    "    print(f\"Completed {pct}% experiment\")\n",
    "\n",
    "print(\"\\nüéâ All experiments completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
