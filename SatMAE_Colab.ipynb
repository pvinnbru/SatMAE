{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cae6c9",
   "metadata": {
    "id": "c6cae6c9"
   },
   "source": [
    "# SatMAE Finetuning on Google Colab\n",
    "\n",
    "This notebook sets up and runs SatMAE finetuning on EuroSAT dataset using Google Colab's free GPU.\n",
    "\n",
    "## üöÄ Features:\n",
    "- Automatic environment setup\n",
    "- EuroSAT dataset download and preprocessing\n",
    "- SatMAE model finetuning with multispectral data\n",
    "- GPU acceleration (T4/V100/A100)\n",
    "\n",
    "**Runtime**: Make sure to select **GPU** runtime (Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811e23a",
   "metadata": {
    "id": "5811e23a"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30d928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b30d928",
    "outputId": "5397e735-dc73-4597-a12c-305e314d3909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "CUDA version: 12.4\n",
      "GPU memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Make sure to enable GPU runtime!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193823a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "193823a0",
    "outputId": "7811b6ae-27c6-478c-acbf-7d833cf2a146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.17)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.4)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.7.14)\n",
      "Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (1.4.3)\n",
      "Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.7.14)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1.2)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.7.14)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "‚úÖ timm version: 1.0.17\n",
      "‚úÖ rasterio version: 1.4.3\n",
      "‚úÖ gdown installed for Google Drive downloads\n",
      "All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install ALL SatMAE requirements (exact versions from env.yml)\n",
    "print(\"üîß Installing SatMAE requirements with exact version compatibility...\")\n",
    "print(\"SatMAE requires PyTorch 1.11.0 + timm 0.3.2 (strict version requirements)\")\n",
    "\n",
    "# CRITICAL: SatMAE has strict version requirements due to API changes\n",
    "# main_finetune.py contains: assert timm.__version__ == \"0.3.2\"\n",
    "# timm 0.3.2 requires PyTorch 1.11.0 (torch._six was removed in newer versions)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Uninstalling current PyTorch to avoid conflicts...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "print(\"\\nüì¶ Installing exact versions required by SatMAE...\")\n",
    "# Install PyTorch 1.11.0 with CUDA 11.3 (matching env.yml exactly)\n",
    "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "\n",
    "# Install timm 0.3.2 (exact version required by SatMAE)\n",
    "!pip install timm==0.3.2\n",
    "\n",
    "# Core ML packages (exact versions from env.yml)\n",
    "!pip install numpy==1.21.2\n",
    "!pip install scipy==1.7.3\n",
    "!pip install matplotlib==3.5.1\n",
    "!pip install pandas==1.3.4\n",
    "!pip install pillow==9.0.1\n",
    "\n",
    "# Geospatial and data packages\n",
    "!pip install rasterio==1.1.0\n",
    "!pip install tqdm==4.63.0\n",
    "\n",
    "# Logging and monitoring\n",
    "!pip install tensorboard==2.6.0\n",
    "!pip install wandb==0.12.14\n",
    "\n",
    "# Additional utilities\n",
    "!pip install pyyaml==6.0\n",
    "!pip install requests==2.27.1\n",
    "\n",
    "print(\"\\nüîç Verifying all installations...\")\n",
    "\n",
    "# Import and verify core packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import timm\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorboard\n",
    "import wandb\n",
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ torchvision version: {torchvision.__version__}\")\n",
    "print(f\"‚úÖ timm version: {timm.__version__}\")\n",
    "print(f\"‚úÖ rasterio version: {rasterio.__version__}\")\n",
    "print(f\"‚úÖ numpy version: {np.__version__}\")\n",
    "print(f\"‚úÖ pandas version: {pd.__version__}\")\n",
    "print(f\"‚úÖ wandb version: {wandb.__version__}\")\n",
    "\n",
    "# Verify the critical requirement\n",
    "if timm.__version__ == \"0.3.2\":\n",
    "    print(\"‚úÖ TIMM version matches SatMAE requirement (0.3.2)\")\n",
    "else:\n",
    "    print(f\"‚ùå TIMM version mismatch! Expected 0.3.2, got {timm.__version__}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"\\nüîç CUDA availability:\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Test critical imports that were failing\n",
    "try:\n",
    "    from timm.models.layers import trunc_normal_\n",
    "    from timm.data.mixup import Mixup\n",
    "    from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "    print(\"‚úÖ Critical timm imports successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Critical timm imports failed: {e}\")\n",
    "\n",
    "print(\"\\nüöÄ All SatMAE requirements ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dec10",
   "metadata": {
    "id": "c91dec10"
   },
   "source": [
    "## 2. Download SatMAE Code and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36bc04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af36bc04",
    "outputId": "b293bdae-7c69-43c7-f685-73bdc9aec78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'SatMAE' already exists and is not an empty directory.\n",
      "/content/SatMAE\n",
      "total 252\n",
      "drwxr-xr-x 7 root root  4096 Jul 25 13:06 .\n",
      "drwxr-xr-x 1 root root  4096 Jul 25 13:06 ..\n",
      "drwxr-xr-x 2 root root  4096 Jul 25 13:06 code_piet\n",
      "-rw-r--r-- 1 root root  6853 Jul 25 13:06 colab_main.py\n",
      "-rw-r--r-- 1 root root  1227 Jul 25 13:06 colab_setup.sh\n",
      "drwxr-xr-x 2 root root  4096 Jul 25 13:06 data_splits\n",
      "-rw-r--r-- 1 root root 10783 Jul 25 13:06 engine_finetune.py\n",
      "-rw-r--r-- 1 root root  6019 Jul 25 13:06 engine_pretrain.py\n",
      "-rw-r--r-- 1 root root  8045 Jul 25 13:06 env.yml\n",
      "-rw-r--r-- 1 root root   563 Jul 25 13:06 execute.txt\n",
      "-rw-r--r-- 1 root root 44308 Jul 25 13:06 file.txt\n",
      "drwxr-xr-x 8 root root  4096 Jul 25 13:06 .git\n",
      "-rw-r--r-- 1 root root    54 Jul 25 13:06 .gitignore\n",
      "drwxr-xr-x 2 root root  4096 Jul 25 13:06 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 root root 19338 Jul 25 13:06 LICENSE\n",
      "-rw-r--r-- 1 root root 20429 Jul 25 13:06 main_finetune.py\n",
      "-rw-r--r-- 1 root root 11542 Jul 25 13:06 main_pretrain.py\n",
      "-rw-r--r-- 1 root root 15775 Jul 25 13:06 models_mae_group_channels.py\n",
      "-rw-r--r-- 1 root root 10080 Jul 25 13:06 models_mae.py\n",
      "-rw-r--r-- 1 root root 14786 Jul 25 13:06 models_mae_temporal.py\n",
      "-rw-r--r-- 1 root root  2201 Jul 25 13:06 models_resnet.py\n",
      "-rw-r--r-- 1 root root  4909 Jul 25 13:06 models_vit_group_channels.py\n",
      "-rw-r--r-- 1 root root  2548 Jul 25 13:06 models_vit.py\n",
      "-rwxr-xr-x 1 root root  3677 Jul 25 13:06 models_vit_temporal.py\n",
      "-rw-r--r-- 1 root root     0 Jul 25 13:06 output.txt\n",
      "-rw-r--r-- 1 root root  3984 Jul 25 13:06 README_Colab.md\n",
      "-rw-r--r-- 1 root root 11084 Jul 25 13:06 README.md\n",
      "-rw-r--r-- 1 root root     0 Jul 25 13:06 SatMAE_Colab.ipynb\n",
      "drwxr-xr-x 4 root root  4096 Jul 25 13:06 util\n"
     ]
    }
   ],
   "source": [
    "# Clone the SatMAE repository\n",
    "!git clone https://github.com/pvinnbru/SatMAE.git\n",
    "%cd SatMAE\n",
    "\n",
    "# List repository contents\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rx3xbWYb4i2K",
   "metadata": {
    "id": "rx3xbWYb4i2K"
   },
   "source": [
    "### üìÅ Google Drive Setup\n",
    "\n",
    "**Required Google Drive Structure:**\n",
    "```\n",
    "MyDrive/\n",
    "‚îú‚îÄ‚îÄ data/                           # Unzipped EuroSAT dataset folder\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ eurosat_ms/                 # Multispectral dataset\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ eurosat_rgb/                # RGB dataset  \n",
    "‚îî‚îÄ‚îÄ checkpoint/\n",
    "    ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth # Pretrained model checkpoint\n",
    "```\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Unzip your EuroSAT dataset and upload the `data/` folder to Google Drive root\n",
    "2. Upload checkpoint to `MyDrive/checkpoint/pretrain-vit-large-e199.pth`\n",
    "3. Run the cells - they will copy files to the local workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wPDQjAL2uzJg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPDQjAL2uzJg",
    "outputId": "38149c2a-b6a9-4585-b5c8-aef732b45fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "‚úÖ Data copied from /content/drive/MyDrive/data to SatMAE/data\n",
      "\n",
      "üìÅ Data structure:\n",
      "total 16\n",
      "drwx------  4 root root 4096 Jul 25 13:33 .\n",
      "drwxr-xr-x  8 root root 4096 Jul 25 13:47 ..\n",
      "drwx------ 12 root root 4096 Jul 25 13:33 eurosat_ms\n",
      "drwx------ 12 root root 4096 Jul 25 13:36 eurosat_rgb\n"
     ]
    }
   ],
   "source": [
    "# Copy data folder from Google Drive\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy data folder from Google Drive to SatMAE/data\n",
    "source = '/content/drive/MyDrive/data'\n",
    "target = 'SatMAE/data'\n",
    "\n",
    "if os.path.exists(target):\n",
    "    shutil.rmtree(target)\n",
    "\n",
    "shutil.copytree(source, target)\n",
    "print(f\"‚úÖ Data copied from {source} to {target}\")\n",
    "\n",
    "# Verify contents\n",
    "print(\"\\nüìÅ Data structure:\")\n",
    "!ls -la SatMAE/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe8df47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfe8df47",
    "outputId": "8ec5166d-33fa-4c37-81ec-438d11a4feae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading pretrained checkpoint from Google Drive...\n",
      "Source: /content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth\n",
      "Target: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "üìÇ Created directory: SatMAE/checkpoints/\n",
      "‚úÖ Found checkpoint in Google Drive\n",
      "üìä File size: 298.8 MB\n",
      "‚úÖ Checkpoint copied successfully!\n",
      "üìÅ Available at: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "‚úÖ Verification successful - checkpoint ready for training\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained checkpoint from Google Drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "drive_checkpoint_path = '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "local_checkpoint_dir = 'SatMAE/checkpoints'\n",
    "local_checkpoint_path = 'SatMAE/checkpoints/pretrain-vit-large-e199.pth'\n",
    "\n",
    "print(\"üîß Loading pretrained checkpoint from Google Drive...\")\n",
    "print(f\"Source: {drive_checkpoint_path}\")\n",
    "print(f\"Target: {local_checkpoint_path}\")\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs(local_checkpoint_dir, exist_ok=True)\n",
    "print(f\"üìÇ Created directory: {local_checkpoint_dir}/\")\n",
    "\n",
    "# Check if checkpoint exists in Google Drive\n",
    "if os.path.exists(drive_checkpoint_path):\n",
    "    print(f\"‚úÖ Found checkpoint in Google Drive\")\n",
    "    print(f\"üìä File size: {os.path.getsize(drive_checkpoint_path) / 1e6:.1f} MB\")\n",
    "\n",
    "    # Copy checkpoint to local directory\n",
    "    try:\n",
    "        shutil.copy2(drive_checkpoint_path, local_checkpoint_path)\n",
    "        print(f\"‚úÖ Checkpoint copied successfully!\")\n",
    "        print(f\"üìÅ Available at: {local_checkpoint_path}\")\n",
    "\n",
    "        # Verify the file\n",
    "        if os.path.exists(local_checkpoint_path):\n",
    "            print(f\"‚úÖ Verification successful - checkpoint ready for training\")\n",
    "        else:\n",
    "            print(f\"‚ùå Verification failed - file not found at target location\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Copy failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint not found at: {drive_checkpoint_path}\")\n",
    "    print(\"Please ensure you have uploaded the checkpoint to your Google Drive\")\n",
    "    print(\"\\nTo fix this:\")\n",
    "    print(\"1. Go to your Google Drive\")\n",
    "    print(\"2. Create a folder called 'checkpoint' in the root directory\")\n",
    "    print(\"3. Upload 'pretrain-vit-large-e199.pth' to MyDrive/checkpoint/\")\n",
    "    print(\"4. Run this cell again\")\n",
    "\n",
    "    print(f\"\\nExpected Google Drive structure:\")\n",
    "    print(f\"  MyDrive/\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ data.zip\")\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ checkpoint/\")\n",
    "    print(f\"      ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e0ad7",
   "metadata": {
    "id": "a83e0ad7"
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EjXuF-fw7KC-",
   "metadata": {
    "id": "EjXuF-fw7KC-"
   },
   "source": [
    "### **3.1 Generate txt Files and Training Subsets**\n",
    "\n",
    "The text files are used for loading Eurosat Data stored in `SatMAE\\data\\`. They look like this:\n",
    "\n",
    "```\n",
    "<path_to_image> <label>\n",
    "```\n",
    "For example:\n",
    "```\n",
    "/path/to/image1.tif    0\n",
    "/path/to/image2.tif    3\n",
    "...\n",
    "```\n",
    "\n",
    "The .txt-files are generate from the script below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eq2bpdtd7LPU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq2bpdtd7LPU",
    "outputId": "4824e4eb-6e4b-4b7d-ad1d-c65021098206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created train/val splits for: SatMAE/data/eurosat_ms\n",
      "   ‚Üí Train: 21600 samples\n",
      "   ‚Üí Val:   5400 samples\n",
      "‚úÖ Created train/val splits for: SatMAE/data/eurosat_rgb\n",
      "   ‚Üí Train: 21600 samples\n",
      "   ‚Üí Val:   5400 samples\n"
     ]
    }
   ],
   "source": [
    "# Create train/val splits and subsets\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "def generate_split_txt(root_folder, out_txt_path, split_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Creates train/val .txt files from a root image folder organized by class.\n",
    "    Supports .tif and .jpg files.\n",
    "    \"\"\"\n",
    "    class_names = sorted(os.listdir(root_folder))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "\n",
    "    all_samples = []\n",
    "    for cls in class_names:\n",
    "        tif_paths = glob(os.path.join(root_folder, cls, \"*.tif\"))\n",
    "        jpg_paths = glob(os.path.join(root_folder, cls, \"*.jpg\"))\n",
    "        image_paths = tif_paths + jpg_paths\n",
    "        for path in image_paths:\n",
    "            all_samples.append(f\"{path} {class_to_idx[cls]}\")\n",
    "\n",
    "    if not all_samples:\n",
    "        print(f\"‚ö†Ô∏è  No image files found in: {root_folder}\")\n",
    "        return\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_samples)\n",
    "    split_idx = int(len(all_samples) * split_ratio)\n",
    "    train_samples = all_samples[:split_idx]\n",
    "    val_samples = all_samples[split_idx:]\n",
    "\n",
    "    with open(out_txt_path.replace(\".txt\", \"_train.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_samples))\n",
    "    with open(out_txt_path.replace(\".txt\", \"_val.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(val_samples))\n",
    "\n",
    "    print(f\"‚úÖ Created train/val splits for: {root_folder}\")\n",
    "    print(f\"   ‚Üí Train: {len(train_samples)} samples\")\n",
    "    print(f\"   ‚Üí Val:   {len(val_samples)} samples\")\n",
    "\n",
    "# Execution\n",
    "generate_split_txt(\"SatMAE/data/eurosat_ms\", \"SatMAE/data_splits/eurosat_ms.txt\")\n",
    "generate_split_txt(\"SatMAE/data/eurosat_rgb\", \"SatMAE/data_splits/eurosat_rgb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SZf5KTQy7NJN",
   "metadata": {
    "id": "SZf5KTQy7NJN"
   },
   "source": [
    "\n",
    "### 3.2 **Create Training Subsets (10%, 25%, 50%, 100%)**\n",
    "\n",
    "The Goal is to measure how model performance improves as the training data size increases. To ensure fair and meaningful comparisons across runs, the validation set remains fixed.\n",
    "\n",
    "The following textfiles were generated and include the complete dataset:\n",
    "\n",
    "```\n",
    "SatMAE/data_splits/eurosat_ms_train.txt\n",
    "SatMAE/data_splits/eurosat_rgb_train.txt\n",
    "```\n",
    "\n",
    "To subsample:\n",
    "\n",
    "* Randomly select a percentage of lines from that file\n",
    "* Save them into new files like:\n",
    "\n",
    "  ```\n",
    "  SatMAE/data_splits/eurosat_ms_train_10.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_25.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_50.txt\n",
    "  ```\n",
    "\n",
    "Do this for RGB and MS too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Sb7t6GON7Pmi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sb7t6GON7Pmi",
    "outputId": "14347e08-e565-4e32-a86a-db62b6c5c23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10% subset to SatMAE/data_splits/eurosat_ms_train_10.txt (2160 samples)\n",
      "Saved 25% subset to SatMAE/data_splits/eurosat_ms_train_25.txt (5400 samples)\n",
      "Saved 50% subset to SatMAE/data_splits/eurosat_ms_train_50.txt (10800 samples)\n",
      "Saved 75% subset to SatMAE/data_splits/eurosat_ms_train_75.txt (16200 samples)\n",
      "Saved 10% subset to SatMAE/data_splits/eurosat_rgb_train_10.txt (2160 samples)\n",
      "Saved 25% subset to SatMAE/data_splits/eurosat_rgb_train_25.txt (5400 samples)\n",
      "Saved 50% subset to SatMAE/data_splits/eurosat_rgb_train_50.txt (10800 samples)\n",
      "Saved 75% subset to SatMAE/data_splits/eurosat_rgb_train_75.txt (16200 samples)\n",
      "\n",
      "‚úÖ Data preprocessing complete!\n",
      "\n",
      "üìÅ Generated files:\n",
      "total 6732\n",
      "drwxr-xr-x 3 root root    4096 Jul 25 14:14 .\n",
      "drwxr-xr-x 9 root root    4096 Jul 25 14:01 ..\n",
      "-rw-r--r-- 1 root root  119137 Jul 25 14:15 eurosat_ms_train_10.txt\n",
      "-rw-r--r-- 1 root root  298594 Jul 25 14:15 eurosat_ms_train_25.txt\n",
      "-rw-r--r-- 1 root root  595927 Jul 25 14:15 eurosat_ms_train_50.txt\n",
      "-rw-r--r-- 1 root root  893882 Jul 25 14:15 eurosat_ms_train_75.txt\n",
      "-rw-r--r-- 1 root root 1191419 Jul 25 14:14 eurosat_ms_train.txt\n",
      "-rw-r--r-- 1 root root  296509 Jul 25 14:14 eurosat_ms_val.txt\n",
      "-rw-r--r-- 1 root root  121255 Jul 25 14:15 eurosat_rgb_train_10.txt\n",
      "-rw-r--r-- 1 root root  303874 Jul 25 14:15 eurosat_rgb_train_25.txt\n",
      "-rw-r--r-- 1 root root  606632 Jul 25 14:15 eurosat_rgb_train_50.txt\n",
      "-rw-r--r-- 1 root root  910021 Jul 25 14:15 eurosat_rgb_train_75.txt\n",
      "-rw-r--r-- 1 root root 1212992 Jul 25 14:14 eurosat_rgb_train.txt\n",
      "-rw-r--r-- 1 root root  301936 Jul 25 14:14 eurosat_rgb_val.txt\n",
      "drwxr-xr-x 2 root root    4096 Jul 25 14:13 .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "def subsample_txt_file(input_path, output_prefix, percentages=[10, 25, 50], seed=42):\n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    for p in percentages:\n",
    "        count = int(len(lines) * (p / 100))\n",
    "        subset = lines[:count]\n",
    "        out_path = f\"{output_prefix}_{p}.txt\"\n",
    "        with open(out_path, 'w') as f_out:\n",
    "            f_out.writelines(subset)\n",
    "        print(f\"Saved {p}% subset to {out_path} ({count} samples)\")\n",
    "\n",
    "\n",
    "#Execution\n",
    "subsample_txt_file(\"SatMAE/data_splits/eurosat_ms_train.txt\", \"SatMAE/data_splits/eurosat_ms_train\", percentages=[10, 25, 50, 75])\n",
    "subsample_txt_file(\"SatMAE/data_splits/eurosat_rgb_train.txt\", \"SatMAE/data_splits/eurosat_rgb_train\", percentages=[10, 25, 50, 75])\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(\"\\nüìÅ Generated files:\")\n",
    "!ls -la SatMAE/data_splits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58938bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify package imports for training\n",
    "print(\"üîç Verifying packages are ready for SatMAE training...\")\n",
    "\n",
    "# All packages should already be installed from Chapter 1\n",
    "# Just verify imports work correctly\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torchvision\n",
    "    import timm\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import sys\n",
    "    import yaml\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"‚úÖ TIMM version: {timm.__version__}\")\n",
    "    print(f\"‚úÖ All imports successful\")\n",
    "    \n",
    "    # Verify the critical requirement\n",
    "    if timm.__version__ == \"0.3.2\":\n",
    "        print(\"‚úÖ TIMM version matches SatMAE requirement (0.3.2)\")\n",
    "    else:\n",
    "        print(f\"‚ùå TIMM version mismatch! Expected 0.3.2, got {timm.__version__}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"‚ö†Ô∏è Please run Chapter 1 package installation first!\")\n",
    "\n",
    "print(\"üöÄ Ready for SatMAE training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5177448",
   "metadata": {
    "id": "f5177448"
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616c4e0",
   "metadata": {
    "id": "e616c4e0"
   },
   "outputs": [],
   "source": [
    "# Verify all required files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'SatMAE/main_finetune.py',\n",
    "    'SatMAE/data_splits/eurosat_ms_train_10.txt',\n",
    "    'SatMAE/data_splits/eurosat_ms_val.txt',\n",
    "    'SatMAE/checkpoints/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "print(\"Checking required files:\")\n",
    "all_good = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüöÄ All files ready for training!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some files are missing. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe43a7",
   "metadata": {
    "id": "a7fe43a7"
   },
   "outputs": [],
   "source": [
    "# Run SatMAE finetuning\n",
    "# Adjust batch_size based on available GPU memory\n",
    "# Check GPU memory and adjust batch size accordingly\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "if gpu_memory_gb >= 15:  # A100, V100\n",
    "    batch_size = 16\n",
    "    accum_iter = 8\n",
    "elif gpu_memory_gb >= 11:  # T4 or similar\n",
    "    batch_size = 8\n",
    "    accum_iter = 16\n",
    "else:  # Smaller GPUs\n",
    "    batch_size = 4\n",
    "    accum_iter = 32\n",
    "\n",
    "print(f\"üöÄ GPU Memory: {gpu_memory_gb:.1f}GB\")\n",
    "print(f\"üöÄ Using batch_size={batch_size}, accum_iter={accum_iter}\")\n",
    "\n",
    "# Change to SatMAE directory and run training command\n",
    "training_cmd = f\"\"\"\n",
    "cd SatMAE && python main_finetune.py \\\n",
    "  --model_type group_c \\\n",
    "  --model vit_large_patch16 \\\n",
    "  --dataset_type euro_sat \\\n",
    "  --train_path data_splits/eurosat_ms_train_10.txt \\\n",
    "  --test_path data_splits/eurosat_ms_val.txt \\\n",
    "  --finetune checkpoints/pretrain-vit-large-e199.pth \\\n",
    "  --input_size 96 --patch_size 8 \\\n",
    "  --batch_size {batch_size} --accum_iter {accum_iter} \\\n",
    "  --epochs 30 --blr 2e-4 \\\n",
    "  --weight_decay 0.05 \\\n",
    "  --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n",
    "  --dropped_bands 0 9 10 \\\n",
    "  --num_workers 2 \\\n",
    "  --output_dir results/eurosat_ms_10 \\\n",
    "  --log_dir results/eurosat_ms_10\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(\"This will take approximately 30-60 minutes depending on GPU\")\n",
    "print(\"Command:\")\n",
    "print(training_cmd)\n",
    "\n",
    "# Execute training\n",
    "!{training_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ea11f",
   "metadata": {
    "id": "938ea11f"
   },
   "source": [
    "## 5. Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb58fcc",
   "metadata": {
    "id": "feb58fcc"
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir SatMAE/results/eurosat_ms_10\n",
    "\n",
    "print(\"TensorBoard is running above!\")\n",
    "print(\"You can monitor training progress, loss curves, and metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7a383",
   "metadata": {
    "id": "64c7a383"
   },
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "import glob\n",
    "\n",
    "results_dir = \"SatMAE/results/eurosat_ms_10\"\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Training results:\")\n",
    "    !ls -la {results_dir}\n",
    "\n",
    "    # Look for log files\n",
    "    log_files = glob.glob(f\"{results_dir}/*.txt\")\n",
    "    if log_files:\n",
    "        print(f\"\\nLatest log file: {log_files[-1]}\")\n",
    "        !tail -20 {log_files[-1]}\n",
    "\n",
    "    # Look for checkpoints\n",
    "    checkpoints = glob.glob(f\"{results_dir}/*.pth\")\n",
    "    if checkpoints:\n",
    "        print(f\"\\nCheckpoints created: {len(checkpoints)}\")\n",
    "        for cp in checkpoints[-3:]:\n",
    "            print(f\"  {cp}\")\n",
    "else:\n",
    "    print(\"No results directory found. Training may not have started yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12aa25",
   "metadata": {
    "id": "ff12aa25"
   },
   "source": [
    "## 6. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de651d",
   "metadata": {
    "id": "41de651d"
   },
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory from SatMAE\n",
    "        results_path = 'SatMAE/results'\n",
    "        if os.path.exists(results_path):\n",
    "            for root, dirs, files in os.walk(results_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add training logs from SatMAE directory\n",
    "        log_files = glob.glob('SatMAE/*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "\n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('SatMAE/results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"‚úÖ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3d0af",
   "metadata": {
    "id": "b2b3d0af"
   },
   "source": [
    "## 7. Optional: Cleanup and Additional Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758d3fd",
   "metadata": {
    "id": "a758d3fd"
   },
   "outputs": [],
   "source": [
    "# Run experiments with different data percentages\n",
    "experiments = [25, 50, 75]\n",
    "\n",
    "for pct in experiments:\n",
    "    print(f\"\\n=== Running experiment with {pct}% of data ===\")\n",
    "\n",
    "    # Adjust epochs based on data size\n",
    "    epochs = max(10, 30 - (pct // 25) * 5)  # Fewer epochs for more data\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    cd SatMAE && python main_finetune.py \\\n",
    "      --model_type group_c \\\n",
    "      --model vit_large_patch16 \\\n",
    "      --dataset_type euro_sat \\\n",
    "      --train_path data_splits/eurosat_ms_train_{pct}.txt \\\n",
    "      --test_path data_splits/eurosat_ms_val.txt \\\n",
    "      --finetune checkpoints/pretrain-vit-large-e199.pth \\\n",
    "      --input_size 96 --patch_size 8 \\\n",
    "      --batch_size {batch_size} --accum_iter {accum_iter} \\\n",
    "      --epochs {epochs} --blr 2e-4 \\\n",
    "      --weight_decay 0.05 \\\n",
    "      --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n",
    "      --dropped_bands 0 9 10 \\\n",
    "      --num_workers 2 \\\n",
    "      --output_dir results/eurosat_ms_{pct} \\\n",
    "      --log_dir results/eurosat_ms_{pct}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training with {pct}% data for {epochs} epochs...\")\n",
    "    !{cmd}\n",
    "\n",
    "    print(f\"Completed {pct}% experiment\")\n",
    "\n",
    "print(\"\\nüéâ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f0b0d",
   "metadata": {
    "id": "fb9f0b0d"
   },
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory from SatMAE\n",
    "        results_path = 'SatMAE/results'\n",
    "        if os.path.exists(results_path):\n",
    "            for root, dirs, files in os.walk(results_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add training logs from SatMAE directory\n",
    "        log_files = glob.glob('SatMAE/*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "\n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('SatMAE/results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"‚úÖ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
