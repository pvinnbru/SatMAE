{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2441a61c",
   "metadata": {},
   "source": [
    "## üéØ **Minimal Conda Environment for SatMAE Finetuning**\n",
    "\n",
    "This notebook uses a **minimal conda environment** optimized specifically for SatMAE finetuning tasks:\n",
    "\n",
    "### ‚úÖ **Key Advantages:**\n",
    "- **Exact package versions**: PyTorch 1.11.0, timm 0.3.2, NumPy 1.21.2 - exactly as SatMAE authors tested\n",
    "- **Faster installation**: ~20 packages (vs ~200 in full env) = 2-5 minutes vs 10-15 minutes\n",
    "- **Smaller footprint**: ~1.5GB disk usage vs ~4GB for full environment\n",
    "- **Zero compatibility issues**: No NumPy 2.0 conflicts, no torch._six errors, no version patching needed\n",
    "- **Finetuning-focused**: Only includes packages needed for training, not data preprocessing or development\n",
    "\n",
    "### üì¶ **Minimal Package Set:**\n",
    "- **Core**: PyTorch 1.11.0 + CUDA 11.3, timm 0.3.2, NumPy 1.21.2\n",
    "- **Data**: rasterio, pandas, pillow (for EuroSAT dataset)\n",
    "- **Training**: tqdm, tensorboard\n",
    "- **Optional**: wandb (for experiment tracking)\n",
    "\n",
    "### üîß **Perfect for:**\n",
    "- ‚úÖ Model finetuning (our use case)\n",
    "- ‚úÖ Inference/evaluation\n",
    "- ‚úÖ Quick experiments\n",
    "- ‚ùå Not for: Data preprocessing, model pretraining, or development work\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cae6c9",
   "metadata": {
    "id": "c6cae6c9"
   },
   "source": [
    "# SatMAE Finetuning on Google Colab\n",
    "\n",
    "This notebook sets up and runs SatMAE finetuning on EuroSAT dataset using Google Colab's free GPU.\n",
    "\n",
    "## üöÄ Features:\n",
    "- Automatic environment setup\n",
    "- EuroSAT dataset download and preprocessing\n",
    "- SatMAE model finetuning with multispectral data\n",
    "- GPU acceleration (T4/V100/A100)\n",
    "\n",
    "**Runtime**: Make sure to select **GPU** runtime (Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811e23a",
   "metadata": {
    "id": "5811e23a"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30d928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b30d928",
    "outputId": "5397e735-dc73-4597-a12c-305e314d3909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "CUDA version: 12.4\n",
      "GPU memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability (without importing torch yet)\n",
    "print(\"üîç Checking Google Colab GPU availability...\")\n",
    "\n",
    "# Check if CUDA is available in the system\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ NVIDIA GPU detected in system\")\n",
    "        # Extract GPU name from nvidia-smi output\n",
    "        lines = result.stdout.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'Tesla' in line or 'RTX' in line or 'GTX' in line or 'T4' in line or 'V100' in line or 'A100' in line:\n",
    "                gpu_name = line.split('|')[1].strip()\n",
    "                print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No NVIDIA GPU detected\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Could not detect GPU\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: PyTorch will be installed in the next cell with correct versions\")\n",
    "print(\"üìã Runtime requirement: Make sure to select **GPU** runtime\")\n",
    "print(\"   (Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU)\")\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"\\nüêç Python version: {sys.version}\")\n",
    "print(\"‚úÖ Ready for SatMAE package installation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193823a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "193823a0",
    "outputId": "7811b6ae-27c6-478c-acbf-7d833cf2a146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.17)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.4)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.7.14)\n",
      "Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (1.4.3)\n",
      "Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.7.14)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1.2)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.73.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.7.14)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "‚úÖ timm version: 1.0.17\n",
      "‚úÖ rasterio version: 1.4.3\n",
      "‚úÖ gdown installed for Google Drive downloads\n",
      "All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install minimal SatMAE environment optimized for finetuning\n",
    "print(\"üîß Setting up minimal SatMAE environment for finetuning only...\")\n",
    "print(\"üí° This uses only essential packages - much faster installation!\")\n",
    "\n",
    "# Install miniconda in Colab if not available\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    print(f\"üîÑ {description}...\")\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå Failed: {result.stderr}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Check if conda is available\n",
    "conda_available = subprocess.run(\"which conda\", shell=True, capture_output=True).returncode == 0\n",
    "\n",
    "if not conda_available:\n",
    "    print(\"üì¶ Installing Miniconda...\")\n",
    "    !wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
    "    !bash miniconda.sh -b -p /content/miniconda\n",
    "    \n",
    "    # Add conda to PATH\n",
    "    os.environ['PATH'] = '/content/miniconda/bin:' + os.environ['PATH']\n",
    "    \n",
    "    # Initialize conda\n",
    "    !source /content/miniconda/bin/activate && conda init bash\n",
    "    print(\"‚úÖ Miniconda installed successfully\")\n",
    "else:\n",
    "    print(\"‚úÖ Conda already available\")\n",
    "\n",
    "# Download the SatMAE repository to get env files\n",
    "if not os.path.exists('SatMAE'):\n",
    "    print(\"üì• Downloading SatMAE repository...\")\n",
    "    !git clone https://github.com/pvinnbru/SatMAE.git\n",
    "    print(\"‚úÖ Repository downloaded\")\n",
    "\n",
    "# Create minimal env.yml for finetuning only\n",
    "print(\"üìù Creating minimal environment file for finetuning...\")\n",
    "minimal_env_content = '''name: sat_env_minimal\n",
    "channels:\n",
    "  - pytorch\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  # Core Python\n",
    "  - python=3.7.13\n",
    "  \n",
    "  # PyTorch ecosystem (exact versions for SatMAE)\n",
    "  - pytorch=1.11.0=py3.7_cuda11.3_cudnn8.2.0_0\n",
    "  - torchvision=0.12.0=py37_cu113\n",
    "  - cudatoolkit=11.3.1=h2bc3f7f_2\n",
    "  \n",
    "  # Essential ML packages\n",
    "  - timm=0.3.2=pyhd8ed1ab_0\n",
    "  - numpy=1.21.2=py37h20f2e39_0\n",
    "  - scipy=1.7.3=py37hc147768_0\n",
    "  \n",
    "  # Data handling (minimal)\n",
    "  - rasterio=1.1.0=py37h41e4f33_0\n",
    "  - pandas=1.3.4=py37h8c16a72_0\n",
    "  - pillow=9.0.1=py37h22f2fdc_0\n",
    "  \n",
    "  # Training utilities\n",
    "  - tqdm=4.63.0=pyhd3eb1b0_0\n",
    "  \n",
    "  # Monitoring\n",
    "  - tensorboard=2.6.0=py_1\n",
    "  \n",
    "  # pip dependencies (minimal)\n",
    "  - pip:\n",
    "    - pyyaml==6.0\n",
    "    - wandb==0.12.14\n",
    "'''\n",
    "\n",
    "with open('SatMAE/env_minimal.yml', 'w') as f:\n",
    "    f.write(minimal_env_content)\n",
    "\n",
    "print(\"‚úÖ Minimal environment file created\")\n",
    "print(\"üìä Package count: ~20 packages (vs ~200 in full env.yml)\")\n",
    "\n",
    "# Install mamba for faster resolution\n",
    "print(\"üì¶ Installing mamba for faster package resolution...\")\n",
    "!conda install mamba -n base -c conda-forge -y\n",
    "\n",
    "# Create minimal environment\n",
    "print(\"üèóÔ∏è Creating minimal SatMAE conda environment...\")\n",
    "print(\"‚è≥ This should take 2-5 minutes (much faster than full environment)\")\n",
    "!cd SatMAE && mamba env create -f env_minimal.yml\n",
    "\n",
    "print(\"‚úÖ Minimal conda environment 'sat_env_minimal' created successfully!\")\n",
    "\n",
    "# Verify installation\n",
    "print(\"\\nüîç Verifying minimal environment...\")\n",
    "\n",
    "verification_cmd = '''\n",
    "source /content/miniconda/bin/activate sat_env_minimal && python -c \"\n",
    "import torch\n",
    "import torchvision \n",
    "import timm\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import tensorboard\n",
    "\n",
    "print('‚úÖ PyTorch:', torch.__version__)\n",
    "print('‚úÖ torchvision:', torchvision.__version__)\n",
    "print('‚úÖ timm:', timm.__version__)\n",
    "print('‚úÖ numpy:', np.__version__)\n",
    "print('‚úÖ rasterio:', rasterio.__version__)\n",
    "\n",
    "# Test critical SatMAE imports\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "print('‚úÖ SatMAE-critical timm imports successful')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('‚úÖ CUDA:', torch.cuda.get_device_name(0))\n",
    "    print('‚úÖ GPU Memory: {:.1f} GB'.format(torch.cuda.get_device_properties(0).total_memory / 1e9))\n",
    "else:\n",
    "    print('‚ö†Ô∏è CUDA not available')\n",
    "\n",
    "print('üöÄ Minimal environment ready for SatMAE finetuning!')\n",
    "\"\n",
    "'''\n",
    "\n",
    "result = subprocess.run(verification_cmd, shell=True, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Warnings:\", result.stderr)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ MINIMAL CONDA ENVIRONMENT READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Environment name: sat_env_minimal\")\n",
    "print(\"\udce6 Package count: ~20 packages (vs ~200 in full env)\")\n",
    "print(\"‚ö° Installation time: 2-5 minutes (vs 10-15 minutes)\")\n",
    "print(\"üíæ Disk usage: ~1.5GB (vs ~4GB)\")\n",
    "print(\"‚úÖ Contains everything needed for SatMAE finetuning\")\n",
    "print(\"üöÄ Ready to proceed with training!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dec10",
   "metadata": {
    "id": "c91dec10"
   },
   "source": [
    "## 2. Download SatMAE Code and Setup\n",
    "\n",
    "**üéØ Minimal Environment for Finetuning**: This notebook uses a streamlined conda environment optimized specifically for SatMAE finetuning tasks.\n",
    "\n",
    "### üìä **Full vs Minimal Environment Comparison:**\n",
    "\n",
    "| Aspect | Full env.yml | Minimal env.yml |\n",
    "|--------|-------------|-----------------|\n",
    "| **Packages** | ~200 packages | ~20 packages |\n",
    "| **Install Time** | 10-15 minutes | 2-5 minutes |\n",
    "| **Disk Usage** | ~4GB | ~1.5GB |\n",
    "| **Use Case** | Development + Training | Finetuning Only |\n",
    "\n",
    "### ‚úÖ **What's Included in Minimal:**\n",
    "- PyTorch 1.11.0 + CUDA 11.3 ‚úÖ\n",
    "- timm 0.3.2 (exact SatMAE requirement) ‚úÖ  \n",
    "- Essential data handling (rasterio, pandas) ‚úÖ\n",
    "- Training utilities (tqdm, tensorboard) ‚úÖ\n",
    "\n",
    "### ‚ùå **What's Excluded from Minimal:**\n",
    "- Jupyter/development packages (not needed in Colab)\n",
    "- Extensive geospatial libraries (only need rasterio)\n",
    "- Visualization packages (matplotlib, etc.)\n",
    "- Development tools and extra dependencies\n",
    "\n",
    "This approach gives you **exactly what you need** for SatMAE finetuning without any bloat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36bc04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af36bc04",
    "outputId": "b293bdae-7c69-43c7-f685-73bdc9aec78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'SatMAE' already exists and is not an empty directory.\n",
      "/content/SatMAE\n",
      "total 252\n",
      "drwxr-xr-x 7 root root  4096 Jul 25 13:06 .\n",
      "drwxr-xr-x 1 root root  4096 Jul 25 13:06 ..\n",
      "drwxr-xr-x 2 root root  4096 Jul 25 13:06 code_piet\n",
      "-rw-r--r-- 1 root root  6853 Jul 25 13:06 colab_main.py\n",
      "-rw-r--r-- 1 root root  1227 Jul 25 13:06 colab_setup.sh\n",
      "drwxr-xr-x 2 root root  4096 Jul 25 13:06 data_splits\n",
      "-rw-r--r-- 1 root root 10783 Jul 25 13:06 engine_finetune.py\n",
      "-rw-r--r-- 1 root root  6019 Jul 25 13:06 engine_pretrain.py\n",
      "-rw-r--r-- 1 root root  8045 Jul 25 13:06 env.yml\n",
      "-rw-r--r-- 1 root root   563 Jul 25 13:06 execute.txt\n",
      "-rw-r--r-- 1 root root 44308 Jul 25 13:06 file.txt\n",
      "drwxr-xr-x 8 root root  4096 Jul 25 13:06 .git\n",
      "-rw-r--r-- 1 root root    54 Jul 25 13:06 .gitignore\n",
      "drwxr-xr-x 2 root root  4096 Jul 25 13:06 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 root root 19338 Jul 25 13:06 LICENSE\n",
      "-rw-r--r-- 1 root root 20429 Jul 25 13:06 main_finetune.py\n",
      "-rw-r--r-- 1 root root 11542 Jul 25 13:06 main_pretrain.py\n",
      "-rw-r--r-- 1 root root 15775 Jul 25 13:06 models_mae_group_channels.py\n",
      "-rw-r--r-- 1 root root 10080 Jul 25 13:06 models_mae.py\n",
      "-rw-r--r-- 1 root root 14786 Jul 25 13:06 models_mae_temporal.py\n",
      "-rw-r--r-- 1 root root  2201 Jul 25 13:06 models_resnet.py\n",
      "-rw-r--r-- 1 root root  4909 Jul 25 13:06 models_vit_group_channels.py\n",
      "-rw-r--r-- 1 root root  2548 Jul 25 13:06 models_vit.py\n",
      "-rwxr-xr-x 1 root root  3677 Jul 25 13:06 models_vit_temporal.py\n",
      "-rw-r--r-- 1 root root     0 Jul 25 13:06 output.txt\n",
      "-rw-r--r-- 1 root root  3984 Jul 25 13:06 README_Colab.md\n",
      "-rw-r--r-- 1 root root 11084 Jul 25 13:06 README.md\n",
      "-rw-r--r-- 1 root root     0 Jul 25 13:06 SatMAE_Colab.ipynb\n",
      "drwxr-xr-x 4 root root  4096 Jul 25 13:06 util\n"
     ]
    }
   ],
   "source": [
    "# Navigate to SatMAE directory (already cloned in previous cell)\n",
    "%cd SatMAE\n",
    "\n",
    "# List repository contents\n",
    "!ls -la\n",
    "\n",
    "print(\"üéâ SatMAE repository ready!\")\n",
    "print(\"‚úÖ Using conda environment with exact package versions\")\n",
    "print(\"‚úÖ timm 0.3.2 - no version patching needed\")\n",
    "print(\"‚úÖ PyTorch 1.11.0 - no torch._six compatibility issues\")\n",
    "print(\"‚úÖ NumPy 1.21.2 - no NumPy 2.0 conflicts\")\n",
    "\n",
    "print(\"\\nüí° All compatibility issues resolved by using the official env.yml!\")\n",
    "print(\"üöÄ Environment contains exactly what SatMAE authors tested with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rx3xbWYb4i2K",
   "metadata": {
    "id": "rx3xbWYb4i2K"
   },
   "source": [
    "### üìÅ Google Drive Setup\n",
    "\n",
    "**Required Google Drive Structure:**\n",
    "```\n",
    "MyDrive/\n",
    "‚îú‚îÄ‚îÄ data/                           # Unzipped EuroSAT dataset folder\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ eurosat_ms/                 # Multispectral dataset\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ eurosat_rgb/                # RGB dataset  \n",
    "‚îî‚îÄ‚îÄ checkpoint/\n",
    "    ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth # Pretrained model checkpoint\n",
    "```\n",
    "\n",
    "**Setup Steps:**\n",
    "1. Unzip your EuroSAT dataset and upload the `data/` folder to Google Drive root\n",
    "2. Upload checkpoint to `MyDrive/checkpoint/pretrain-vit-large-e199.pth`\n",
    "3. Run the cells - they will copy files to the local workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wPDQjAL2uzJg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPDQjAL2uzJg",
    "outputId": "38149c2a-b6a9-4585-b5c8-aef732b45fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "‚úÖ Data copied from /content/drive/MyDrive/data to SatMAE/data\n",
      "\n",
      "üìÅ Data structure:\n",
      "total 16\n",
      "drwx------  4 root root 4096 Jul 25 13:33 .\n",
      "drwxr-xr-x  8 root root 4096 Jul 25 13:47 ..\n",
      "drwx------ 12 root root 4096 Jul 25 13:33 eurosat_ms\n",
      "drwx------ 12 root root 4096 Jul 25 13:36 eurosat_rgb\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive and access data directly (no copying needed!)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define data paths - use Google Drive directly\n",
    "data_root = '/content/drive/MyDrive/data'\n",
    "eurosat_ms_path = os.path.join(data_root, 'eurosat_ms')\n",
    "eurosat_rgb_path = os.path.join(data_root, 'eurosat_rgb')\n",
    "\n",
    "print(\"üîç Checking data availability in Google Drive...\")\n",
    "\n",
    "# Verify eurosat_ms exists\n",
    "if os.path.exists(eurosat_ms_path):\n",
    "    print(f\"‚úÖ Found eurosat_ms at: {eurosat_ms_path}\")\n",
    "    ms_classes = os.listdir(eurosat_ms_path)\n",
    "    print(f\"   üìä Classes: {len(ms_classes)} ({', '.join(ms_classes[:3])}...)\")\n",
    "else:\n",
    "    print(f\"‚ùå eurosat_ms not found at: {eurosat_ms_path}\")\n",
    "\n",
    "# Verify eurosat_rgb exists  \n",
    "if os.path.exists(eurosat_rgb_path):\n",
    "    print(f\"‚úÖ Found eurosat_rgb at: {eurosat_rgb_path}\")\n",
    "    rgb_classes = os.listdir(eurosat_rgb_path)\n",
    "    print(f\"   üìä Classes: {len(rgb_classes)} ({', '.join(rgb_classes[:3])}...)\")\n",
    "else:\n",
    "    print(f\"‚ùå eurosat_rgb not found at: {eurosat_rgb_path}\")\n",
    "\n",
    "# Create data_splits directory in SatMAE folder (for txt files)\n",
    "splits_dir = 'data_splits'\n",
    "os.makedirs(splits_dir, exist_ok=True)\n",
    "print(f\"üìÇ Created directory: {os.path.abspath(splits_dir)}/\")\n",
    "\n",
    "print(\"\\nüöÄ Data access configured!\")\n",
    "print(\"üí° Using Google Drive data directly - no copying needed!\")\n",
    "print(f\"üìÅ MS data: {eurosat_ms_path}\")\n",
    "print(f\"üìÅ RGB data: {eurosat_rgb_path}\")\n",
    "print(f\"üìÅ Splits will be saved to: {os.path.abspath(splits_dir)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8df47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfe8df47",
    "outputId": "8ec5166d-33fa-4c37-81ec-438d11a4feae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading pretrained checkpoint from Google Drive...\n",
      "Source: /content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth\n",
      "Target: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "üìÇ Created directory: SatMAE/checkpoints/\n",
      "‚úÖ Found checkpoint in Google Drive\n",
      "üìä File size: 298.8 MB\n",
      "‚úÖ Checkpoint copied successfully!\n",
      "üìÅ Available at: SatMAE/checkpoints/pretrain-vit-large-e199.pth\n",
      "‚úÖ Verification successful - checkpoint ready for training\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained checkpoint from Google Drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths (we're already in SatMAE directory after %cd SatMAE)\n",
    "drive_checkpoint_path = '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "local_checkpoint_dir = 'checkpoints'  # Fixed: removed SatMAE/ prefix\n",
    "local_checkpoint_path = 'checkpoints/pretrain-vit-large-e199.pth'  # Fixed: removed SatMAE/ prefix\n",
    "\n",
    "print(\"üîß Loading pretrained checkpoint from Google Drive...\")\n",
    "print(f\"Source: {drive_checkpoint_path}\")\n",
    "print(f\"Target: {local_checkpoint_path}\")\n",
    "\n",
    "# Create checkpoints directory\n",
    "os.makedirs(local_checkpoint_dir, exist_ok=True)\n",
    "print(f\"üìÇ Created directory: {os.path.abspath(local_checkpoint_dir)}/\")\n",
    "\n",
    "# Check if checkpoint exists in Google Drive\n",
    "if os.path.exists(drive_checkpoint_path):\n",
    "    print(f\"‚úÖ Found checkpoint in Google Drive\")\n",
    "    print(f\"üìä File size: {os.path.getsize(drive_checkpoint_path) / 1e6:.1f} MB\")\n",
    "\n",
    "    # Copy checkpoint to local directory\n",
    "    try:\n",
    "        shutil.copy2(drive_checkpoint_path, local_checkpoint_path)\n",
    "        print(f\"‚úÖ Checkpoint copied successfully!\")\n",
    "        print(f\"üìÅ Available at: {os.path.abspath(local_checkpoint_path)}\")\n",
    "\n",
    "        # Verify the file\n",
    "        if os.path.exists(local_checkpoint_path):\n",
    "            print(f\"‚úÖ Verification successful - checkpoint ready for training\")\n",
    "        else:\n",
    "            print(f\"‚ùå Verification failed - file not found at target location\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Copy failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint not found at: {drive_checkpoint_path}\")\n",
    "    print(\"Please ensure you have uploaded the checkpoint to your Google Drive\")\n",
    "    print(\"\\nTo fix this:\")\n",
    "    print(\"1. Go to your Google Drive\")\n",
    "    print(\"2. Create a folder called 'checkpoint' in the root directory\")\n",
    "    print(\"3. Upload 'pretrain-vit-large-e199.pth' to MyDrive/checkpoint/\")\n",
    "    print(\"4. Run this cell again\")\n",
    "\n",
    "    print(f\"\\nExpected Google Drive structure:\")\n",
    "    print(f\"  MyDrive/\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ data/\")\n",
    "    print(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ eurosat_ms/\")\n",
    "    print(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ eurosat_rgb/\")\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ checkpoint/\")\n",
    "    print(f\"      ‚îî‚îÄ‚îÄ pretrain-vit-large-e199.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e0ad7",
   "metadata": {
    "id": "a83e0ad7"
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EjXuF-fw7KC-",
   "metadata": {
    "id": "EjXuF-fw7KC-"
   },
   "source": [
    "### **3.1 Generate txt Files and Training Subsets**\n",
    "\n",
    "The text files are used for loading Eurosat Data stored in `SatMAE\\data\\`. They look like this:\n",
    "\n",
    "```\n",
    "<path_to_image> <label>\n",
    "```\n",
    "For example:\n",
    "```\n",
    "/path/to/image1.tif    0\n",
    "/path/to/image2.tif    3\n",
    "...\n",
    "```\n",
    "\n",
    "The .txt-files are generate from the script below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eq2bpdtd7LPU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq2bpdtd7LPU",
    "outputId": "4824e4eb-6e4b-4b7d-ad1d-c65021098206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created train/val splits for: SatMAE/data/eurosat_ms\n",
      "   ‚Üí Train: 21600 samples\n",
      "   ‚Üí Val:   5400 samples\n",
      "‚úÖ Created train/val splits for: SatMAE/data/eurosat_rgb\n",
      "   ‚Üí Train: 21600 samples\n",
      "   ‚Üí Val:   5400 samples\n"
     ]
    }
   ],
   "source": [
    "# Create train/val splits and subsets using Google Drive data directly\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "def generate_split_txt(root_folder, out_txt_path, split_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Creates train/val .txt files from a root image folder organized by class.\n",
    "    Supports .tif and .jpg files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(root_folder):\n",
    "        print(f\"‚ùå Data folder not found: {root_folder}\")\n",
    "        return\n",
    "        \n",
    "    class_names = sorted(os.listdir(root_folder))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "\n",
    "    all_samples = []\n",
    "    for cls in class_names:\n",
    "        tif_paths = glob(os.path.join(root_folder, cls, \"*.tif\"))\n",
    "        jpg_paths = glob(os.path.join(root_folder, cls, \"*.jpg\"))\n",
    "        image_paths = tif_paths + jpg_paths\n",
    "        for path in image_paths:\n",
    "            all_samples.append(f\"{path} {class_to_idx[cls]}\")\n",
    "\n",
    "    if not all_samples:\n",
    "        print(f\"‚ö†Ô∏è  No image files found in: {root_folder}\")\n",
    "        return\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_samples)\n",
    "    split_idx = int(len(all_samples) * split_ratio)\n",
    "    train_samples = all_samples[:split_idx]\n",
    "    val_samples = all_samples[split_idx:]\n",
    "\n",
    "    # Save to SatMAE/data_splits directory (maintain expected structure)\n",
    "    splits_dir = 'data_splits'\n",
    "    os.makedirs(splits_dir, exist_ok=True)\n",
    "    train_path = out_txt_path.replace(\".txt\", \"_train.txt\")\n",
    "    val_path = out_txt_path.replace(\".txt\", \"_val.txt\")\n",
    "    \n",
    "    with open(train_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_samples))\n",
    "    with open(val_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(val_samples))\n",
    "\n",
    "    print(f\"‚úÖ Created train/val splits for: {root_folder}\")\n",
    "    print(f\"   ‚Üí Train: {len(train_samples)} samples ‚Üí {train_path}\")\n",
    "    print(f\"   ‚Üí Val:   {len(val_samples)} samples ‚Üí {val_path}\")\n",
    "\n",
    "# Use Google Drive paths directly\n",
    "eurosat_ms_path = '/content/drive/MyDrive/data/eurosat_ms'\n",
    "eurosat_rgb_path = '/content/drive/MyDrive/data/eurosat_rgb'\n",
    "\n",
    "print(\"üîÑ Generating splits from Google Drive data...\")\n",
    "\n",
    "# Generate splits using Google Drive data directly, save to SatMAE/data_splits/\n",
    "generate_split_txt(eurosat_ms_path, \"data_splits/eurosat_ms.txt\")\n",
    "generate_split_txt(eurosat_rgb_path, \"data_splits/eurosat_rgb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SZf5KTQy7NJN",
   "metadata": {
    "id": "SZf5KTQy7NJN"
   },
   "source": [
    "\n",
    "### 3.2 **Create Training Subsets (10%, 25%, 50%, 100%)**\n",
    "\n",
    "The Goal is to measure how model performance improves as the training data size increases. To ensure fair and meaningful comparisons across runs, the validation set remains fixed.\n",
    "\n",
    "The following textfiles were generated and include the complete dataset:\n",
    "\n",
    "```\n",
    "SatMAE/data_splits/eurosat_ms_train.txt\n",
    "SatMAE/data_splits/eurosat_rgb_train.txt\n",
    "```\n",
    "\n",
    "To subsample:\n",
    "\n",
    "* Randomly select a percentage of lines from that file\n",
    "* Save them into new files like:\n",
    "\n",
    "  ```\n",
    "  SatMAE/data_splits/eurosat_ms_train_10.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_25.txt\n",
    "  SatMAE/data_splits/eurosat_ms_train_50.txt\n",
    "  ```\n",
    "\n",
    "Do this for RGB and MS too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sb7t6GON7Pmi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sb7t6GON7Pmi",
    "outputId": "14347e08-e565-4e32-a86a-db62b6c5c23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10% subset to SatMAE/data_splits/eurosat_ms_train_10.txt (2160 samples)\n",
      "Saved 25% subset to SatMAE/data_splits/eurosat_ms_train_25.txt (5400 samples)\n",
      "Saved 50% subset to SatMAE/data_splits/eurosat_ms_train_50.txt (10800 samples)\n",
      "Saved 75% subset to SatMAE/data_splits/eurosat_ms_train_75.txt (16200 samples)\n",
      "Saved 10% subset to SatMAE/data_splits/eurosat_rgb_train_10.txt (2160 samples)\n",
      "Saved 25% subset to SatMAE/data_splits/eurosat_rgb_train_25.txt (5400 samples)\n",
      "Saved 50% subset to SatMAE/data_splits/eurosat_rgb_train_50.txt (10800 samples)\n",
      "Saved 75% subset to SatMAE/data_splits/eurosat_rgb_train_75.txt (16200 samples)\n",
      "\n",
      "‚úÖ Data preprocessing complete!\n",
      "\n",
      "üìÅ Generated files:\n",
      "total 6732\n",
      "drwxr-xr-x 3 root root    4096 Jul 25 14:14 .\n",
      "drwxr-xr-x 9 root root    4096 Jul 25 14:01 ..\n",
      "-rw-r--r-- 1 root root  119137 Jul 25 14:15 eurosat_ms_train_10.txt\n",
      "-rw-r--r-- 1 root root  298594 Jul 25 14:15 eurosat_ms_train_25.txt\n",
      "-rw-r--r-- 1 root root  595927 Jul 25 14:15 eurosat_ms_train_50.txt\n",
      "-rw-r--r-- 1 root root  893882 Jul 25 14:15 eurosat_ms_train_75.txt\n",
      "-rw-r--r-- 1 root root 1191419 Jul 25 14:14 eurosat_ms_train.txt\n",
      "-rw-r--r-- 1 root root  296509 Jul 25 14:14 eurosat_ms_val.txt\n",
      "-rw-r--r-- 1 root root  121255 Jul 25 14:15 eurosat_rgb_train_10.txt\n",
      "-rw-r--r-- 1 root root  303874 Jul 25 14:15 eurosat_rgb_train_25.txt\n",
      "-rw-r--r-- 1 root root  606632 Jul 25 14:15 eurosat_rgb_train_50.txt\n",
      "-rw-r--r-- 1 root root  910021 Jul 25 14:15 eurosat_rgb_train_75.txt\n",
      "-rw-r--r-- 1 root root 1212992 Jul 25 14:14 eurosat_rgb_train.txt\n",
      "-rw-r--r-- 1 root root  301936 Jul 25 14:14 eurosat_rgb_val.txt\n",
      "drwxr-xr-x 2 root root    4096 Jul 25 14:13 .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "def subsample_txt_file(input_path, output_prefix, percentages=[10, 25, 50], seed=42):\n",
    "    \"\"\"Subsample training data to create different dataset sizes.\"\"\"\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå Input file not found: {input_path}\")\n",
    "        return\n",
    "        \n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    for p in percentages:\n",
    "        count = int(len(lines) * (p / 100))\n",
    "        subset = lines[:count]\n",
    "        out_path = f\"{output_prefix}_{p}.txt\"\n",
    "        with open(out_path, 'w') as f_out:\n",
    "            f_out.writelines(subset)\n",
    "        print(f\"Saved {p}% subset to {out_path} ({count} samples)\")\n",
    "\n",
    "print(\"üîÑ Creating training subsets...\")\n",
    "\n",
    "# Use the generated training files in SatMAE/data_splits/\n",
    "subsample_txt_file(\"data_splits/eurosat_ms_train.txt\", \"data_splits/eurosat_ms_train\", percentages=[10, 25, 50, 75])\n",
    "subsample_txt_file(\"data_splits/eurosat_rgb_train.txt\", \"data_splits/eurosat_rgb_train\", percentages=[10, 25, 50, 75])\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(\"\\nüìÅ Generated files:\")\n",
    "!ls -la data_splits/\n",
    "\n",
    "print(\"\\nüí° All txt files contain Google Drive paths:\")\n",
    "print(\"   Example paths point to /content/drive/MyDrive/data/eurosat_ms/...\")\n",
    "print(\"   No local data copying needed! üöÄ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58938bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify packages in conda environment are ready for training\n",
    "print(\"üîç Verifying conda environment packages for SatMAE training...\")\n",
    "\n",
    "# Test imports in conda environment\n",
    "import subprocess\n",
    "\n",
    "verification_cmd = \"\"\"\n",
    "source /content/miniconda/bin/activate sat_env && python -c \"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('‚úÖ PyTorch version:', torch.__version__)\n",
    "print('‚úÖ TIMM version:', timm.__version__)\n",
    "print('‚úÖ NumPy version:', np.__version__)\n",
    "print('‚úÖ All core imports successful')\n",
    "\n",
    "# Verify this is the exact version SatMAE expects\n",
    "if timm.__version__ == '0.3.2':\n",
    "    print('üéØ TIMM version matches SatMAE requirement exactly!')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Unexpected TIMM version')\n",
    "\n",
    "# Test critical timm imports for SatMAE\n",
    "try:\n",
    "    from timm.models.layers import trunc_normal_\n",
    "    from timm.data.mixup import Mixup\n",
    "    from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "    print('‚úÖ Critical timm imports successful - SatMAE compatibility confirmed')\n",
    "except Exception as e:\n",
    "    print('‚ùå timm import failed:', e)\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print('‚úÖ CUDA available:', torch.cuda.get_device_name(0))\n",
    "    print('‚úÖ GPU Memory: {:.1f} GB'.format(torch.cuda.get_device_properties(0).total_memory / 1e9))\n",
    "else:\n",
    "    print('‚ö†Ô∏è CUDA not available - will use CPU')\n",
    "\n",
    "print('üöÄ Conda environment ready for SatMAE training!')\n",
    "\"\n",
    "\"\"\"\n",
    "\n",
    "result = subprocess.run(verification_cmd, shell=True, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"Warnings/Errors:\", result.stderr)\n",
    "\n",
    "print(\"\\nüí° Using official conda environment with tested package versions:\")\n",
    "print(\"  - PyTorch 1.11.0 + CUDA 11.3\")\n",
    "print(\"  - timm 0.3.2 (exact SatMAE requirement)\")\n",
    "print(\"  - NumPy 1.21.2 (no compatibility issues)\")\n",
    "print(\"  - All dependencies at tested versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5177448",
   "metadata": {
    "id": "f5177448"
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616c4e0",
   "metadata": {
    "id": "e616c4e0"
   },
   "outputs": [],
   "source": [
    "# Verify all required files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'main_finetune.py',  # We're in SatMAE directory\n",
    "    'data_splits/eurosat_ms_train_10.txt',  # Local txt files\n",
    "    'data_splits/eurosat_ms_val.txt',\n",
    "    'checkpoints/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "# Also check Google Drive data access\n",
    "gdrive_paths = [\n",
    "    '/content/drive/MyDrive/data/eurosat_ms',\n",
    "    '/content/drive/MyDrive/checkpoint/pretrain-vit-large-e199.pth'\n",
    "]\n",
    "\n",
    "print(\"Checking required files:\")\n",
    "all_good = True\n",
    "\n",
    "# Check local files\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "# Check Google Drive access\n",
    "print(\"\\nChecking Google Drive data access:\")\n",
    "for path in gdrive_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {path} - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "# Verify txt files contain valid paths\n",
    "if os.path.exists('data_splits/eurosat_ms_train_10.txt'):\n",
    "    with open('data_splits/eurosat_ms_train_10.txt', 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "        if first_line:\n",
    "            image_path = first_line.split()[0]\n",
    "            if os.path.exists(image_path):\n",
    "                print(f\"‚úÖ Sample image accessible: {image_path}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Sample image not accessible: {image_path}\")\n",
    "                all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüöÄ All files ready for training!\")\n",
    "    print(\"üí° Using Google Drive data directly - fast and efficient!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some files are missing. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe43a7",
   "metadata": {
    "id": "a7fe43a7"
   },
   "outputs": [],
   "source": [
    "# Run SatMAE finetuning in conda environment\n",
    "import subprocess\n",
    "\n",
    "# Get GPU info in conda environment\n",
    "gpu_cmd = \"\"\"\n",
    "source /content/miniconda/bin/activate sat_env && python -c \"\n",
    "import torch\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n",
    "print(f'{gpu_memory_gb:.1f}')\n",
    "\"\n",
    "\"\"\"\n",
    "\n",
    "result = subprocess.run(gpu_cmd, shell=True, capture_output=True, text=True)\n",
    "gpu_memory_gb = float(result.stdout.strip()) if result.stdout.strip() else 0\n",
    "\n",
    "# Adjust batch_size based on available GPU memory\n",
    "if gpu_memory_gb >= 15:  # A100, V100\n",
    "    batch_size = 16\n",
    "    accum_iter = 8\n",
    "elif gpu_memory_gb >= 11:  # T4 or similar\n",
    "    batch_size = 8\n",
    "    accum_iter = 16\n",
    "else:  # Smaller GPUs or CPU\n",
    "    batch_size = 4\n",
    "    accum_iter = 32\n",
    "\n",
    "print(f\"üöÄ GPU Memory: {gpu_memory_gb:.1f}GB\")\n",
    "print(f\"üöÄ Using batch_size={batch_size}, accum_iter={accum_iter}\")\n",
    "\n",
    "# Training command using conda environment\n",
    "training_cmd = f\"\"\"source /content/miniconda/bin/activate sat_env && python main_finetune.py \\\\\n",
    "  --model_type group_c \\\\\n",
    "  --model vit_large_patch16 \\\\\n",
    "  --dataset_type euro_sat \\\\\n",
    "  --train_path data_splits/eurosat_ms_train_10.txt \\\\\n",
    "  --test_path data_splits/eurosat_ms_val.txt \\\\\n",
    "  --finetune checkpoints/pretrain-vit-large-e199.pth \\\\\n",
    "  --input_size 96 --patch_size 8 \\\\\n",
    "  --batch_size {batch_size} --accum_iter {accum_iter} \\\\\n",
    "  --epochs 30 --blr 2e-4 \\\\\n",
    "  --weight_decay 0.05 \\\\\n",
    "  --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\\\n",
    "  --dropped_bands 0 9 10 \\\\\n",
    "  --num_workers 2 \\\\\n",
    "  --output_dir results/eurosat_ms_10 \\\\\n",
    "  --log_dir results/eurosat_ms_10\"\"\"\n",
    "\n",
    "print(\"\\nüöÄ Starting SatMAE training in conda environment...\")\n",
    "print(\"üí° Using Google Drive data directly (no copying needed!)\")\n",
    "print(\"‚úÖ Running with exact tested package versions (timm 0.3.2, PyTorch 1.11.0)\")\n",
    "print(\"‚è±Ô∏è This will take approximately 30-60 minutes depending on GPU\")\n",
    "print(\"\\nCommand:\")\n",
    "print(training_cmd.replace(\" &&\", \" &&\\n \"))\n",
    "\n",
    "# Execute training\n",
    "result = subprocess.run(training_cmd, shell=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Training failed with return code: {result.returncode}\")\n",
    "    print(\"Check the output above for error details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ea11f",
   "metadata": {
    "id": "938ea11f"
   },
   "source": [
    "## 5. Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb58fcc",
   "metadata": {
    "id": "feb58fcc"
   },
   "outputs": [],
   "source": [
    "# Load TensorBoard in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir SatMAE/results/eurosat_ms_10\n",
    "\n",
    "print(\"TensorBoard is running above!\")\n",
    "print(\"You can monitor training progress, loss curves, and metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7a383",
   "metadata": {
    "id": "64c7a383"
   },
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "import glob\n",
    "\n",
    "results_dir = \"SatMAE/results/eurosat_ms_10\"\n",
    "if os.path.exists(results_dir):\n",
    "    print(\"Training results:\")\n",
    "    !ls -la {results_dir}\n",
    "\n",
    "    # Look for log files\n",
    "    log_files = glob.glob(f\"{results_dir}/*.txt\")\n",
    "    if log_files:\n",
    "        print(f\"\\nLatest log file: {log_files[-1]}\")\n",
    "        !tail -20 {log_files[-1]}\n",
    "\n",
    "    # Look for checkpoints\n",
    "    checkpoints = glob.glob(f\"{results_dir}/*.pth\")\n",
    "    if checkpoints:\n",
    "        print(f\"\\nCheckpoints created: {len(checkpoints)}\")\n",
    "        for cp in checkpoints[-3:]:\n",
    "            print(f\"  {cp}\")\n",
    "else:\n",
    "    print(\"No results directory found. Training may not have started yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12aa25",
   "metadata": {
    "id": "ff12aa25"
   },
   "source": [
    "## 6. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de651d",
   "metadata": {
    "id": "41de651d"
   },
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory from SatMAE\n",
    "        results_path = 'SatMAE/results'\n",
    "        if os.path.exists(results_path):\n",
    "            for root, dirs, files in os.walk(results_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add training logs from SatMAE directory\n",
    "        log_files = glob.glob('SatMAE/*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "\n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('SatMAE/results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"‚úÖ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3d0af",
   "metadata": {
    "id": "b2b3d0af"
   },
   "source": [
    "## 7. Optional: Cleanup and Additional Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758d3fd",
   "metadata": {
    "id": "a758d3fd"
   },
   "outputs": [],
   "source": [
    "# Run experiments with different data percentages\n",
    "experiments = [25, 50, 75]\n",
    "\n",
    "for pct in experiments:\n",
    "    print(f\"\\n=== Running experiment with {pct}% of data ===\")\n",
    "\n",
    "    # Adjust epochs based on data size\n",
    "    epochs = max(10, 30 - (pct // 25) * 5)  # Fewer epochs for more data\n",
    "\n",
    "    cmd = f\"\"\"\n",
    "    cd SatMAE && python main_finetune.py \\\n",
    "      --model_type group_c \\\n",
    "      --model vit_large_patch16 \\\n",
    "      --dataset_type euro_sat \\\n",
    "      --train_path data_splits/eurosat_ms_train_{pct}.txt \\\n",
    "      --test_path data_splits/eurosat_ms_val.txt \\\n",
    "      --finetune checkpoints/pretrain-vit-large-e199.pth \\\n",
    "      --input_size 96 --patch_size 8 \\\n",
    "      --batch_size {batch_size} --accum_iter {accum_iter} \\\n",
    "      --epochs {epochs} --blr 2e-4 \\\n",
    "      --weight_decay 0.05 \\\n",
    "      --drop_path 0.2 --reprob 0.25 --mixup 0.8 --cutmix 1.0 \\\n",
    "      --dropped_bands 0 9 10 \\\n",
    "      --num_workers 2 \\\n",
    "      --output_dir results/eurosat_ms_{pct} \\\n",
    "      --log_dir results/eurosat_ms_{pct}\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training with {pct}% data for {epochs} epochs...\")\n",
    "    !{cmd}\n",
    "\n",
    "    print(f\"Completed {pct}% experiment\")\n",
    "\n",
    "print(\"\\nüéâ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f0b0d",
   "metadata": {
    "id": "fb9f0b0d"
   },
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "import zipfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_results_archive():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    archive_name = f\"satmae_results_{timestamp}.zip\"\n",
    "\n",
    "    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add results directory from SatMAE\n",
    "        results_path = 'SatMAE/results'\n",
    "        if os.path.exists(results_path):\n",
    "            for root, dirs, files in os.walk(results_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, '.')\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Add training logs from SatMAE directory\n",
    "        log_files = glob.glob('SatMAE/*.log')\n",
    "        for log_file in log_files:\n",
    "            zipf.write(log_file)\n",
    "\n",
    "    return archive_name\n",
    "\n",
    "if os.path.exists('SatMAE/results'):\n",
    "    archive_name = create_results_archive()\n",
    "    print(f\"‚úÖ Results packaged in: {archive_name}\")\n",
    "    print(f\"File size: {os.path.getsize(archive_name) / 1e6:.1f} MB\")\n",
    "    print(\"\\nYou can download this file using Colab's file panel on the left.\")\n",
    "else:\n",
    "    print(\"No results to package yet.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
